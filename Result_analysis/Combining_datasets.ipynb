{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source activate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import  partial\n",
    "sys.path.insert(0, '/cluster/work/climate/dnikolo/n2o')\n",
    "from Glaciation_time_estimator.Data_postprocessing.Job_result_fp_generator import generate_tracking_filenames\n",
    "from Glaciation_time_estimator.Auxiliary_func.config_reader import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(\n",
    "    '/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/config_half.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_array_from_df(series: pd.Series):\n",
    "    if series.empty:\n",
    "        return None\n",
    "    return np.stack(series.values)\n",
    "\n",
    "def get_glaciations_df(config):\n",
    "    agg_fact = config['agg_fact']\n",
    "    folder_name = f\"{config['start_time'].strftime(config['time_folder_format'])}_{config['end_time'].strftime(config['time_folder_format'])}\"\n",
    "    pole=config[\"pole_folders\"][0]\n",
    "    fp = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                pole,\n",
    "                folder_name,\n",
    "                f\"Agg_{agg_fact:02}_Glaciations.parquet\"\n",
    "            )\n",
    "    try:\n",
    "        return pd.read_parquet(fp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping glaciations\")\n",
    "        return \n",
    "\n",
    "def get_combined_cloud_df(config):\n",
    "    t_deltas = config['t_deltas']\n",
    "    agg_fact = config['agg_fact']\n",
    "    min_temp_array, max_temp_array = config['min_temp_arr'], config['max_temp_arr']\n",
    "    folder_name = f\"{config['start_time'].strftime(config['time_folder_format'])}_{config['end_time'].strftime(config['time_folder_format'])}\"\n",
    "    # Initialize an empty list to store the individual dataframes\n",
    "    cloud_properties_df_list = []\n",
    "\n",
    "    # Iterate over each temperature range\n",
    "    for i in range(len(min_temp_array)):\n",
    "        cloud_properties_df_list.append([])\n",
    "        min_temp = min_temp_array[i]\n",
    "        max_temp = max_temp_array[i]\n",
    "\n",
    "        # Iterate over each pole\n",
    "        for pole in config[\"pole_folders\"]:\n",
    "            # Construct the file path\n",
    "            fp = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                pole,\n",
    "                folder_name,\n",
    "                f\"Agg_{agg_fact:02}_T_{abs(round(min_temp)):02}_{abs(round(max_temp)):02}.parquet\"\n",
    "            )\n",
    "\n",
    "            # Read the parquet file into a dataframe\n",
    "            try:\n",
    "                df = pd.read_parquet(fp)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Skipping all clouds file: {pole} {min_temp} to {max_temp}\")\n",
    "                continue\n",
    "\n",
    "            # Add columns for min_temp, max_temp, and pole\n",
    "            df['min_temp'] = min_temp\n",
    "            df['max_temp'] = max_temp\n",
    "            df['pole'] = pole\n",
    "            df['Hemisphere'] = \"South\" if pole == \"sp\" else \"North\"\n",
    "            df['Lifetime [h]'] = df['track_length'] / pd.Timedelta(hours=1)\n",
    "            df[\"Radius [km]\"]=np.sqrt(df[\"avg_size[km]\"]/np.pi)\n",
    "            # Append the dataframe to the sublist\n",
    "            cloud_properties_df_list[i].append(df)\n",
    "\n",
    "    # Combine all dataframes into a single dataframe\n",
    "    if len(cloud_properties_df_list)==0:\n",
    "        return None\n",
    "    return pd.concat(\n",
    "        [df for sublist in cloud_properties_df_list for df in sublist], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasify_clouds(yearly_data):\n",
    "    yearly_data[\"Level\"] = pd.cut(\n",
    "        yearly_data.avg_ctp,\n",
    "        bins=[50, 440, 680, 1000],\n",
    "        labels=[\"Cirro\",\"Alto\",\"Low\"]\n",
    "    )\n",
    "    yearly_data[\"Optical Thickness\"] = pd.cut(\n",
    "        yearly_data.avg_cot,\n",
    "        bins=[0, 3.6, 23, 379],\n",
    "        labels=[\"Thin\", \"Medium\", \"Thick\"]\n",
    "    )\n",
    "\n",
    "    yearly_data[\"Cloud type\"] = list(zip(yearly_data[\"Level\"],yearly_data[\"Optical Thickness\"]))\n",
    "    # Define mapping dictionary\n",
    "    cloud_type_mapping = {\n",
    "        (\"Low\", \"Thin\"): \"Cumulus\",\n",
    "        (\"Alto\", \"Thin\"): \"Altocumulus\",\n",
    "        (\"Cirro\", \"Thin\"): \"Cirrus\",\n",
    "        (\"Low\", \"Medium\"): \"Stratocumulus\",\n",
    "        (\"Alto\", \"Medium\"): \"Altostratus\",\n",
    "        (\"Cirro\", \"Medium\"): \"Cirrostratus\",\n",
    "        (\"Low\", \"Thick\"): \"Stratus\",\n",
    "        (\"Alto\", \"Thick\"): \"Nimbostratus\",\n",
    "        (\"Cirro\", \"Thick\"): \"Deep convection\",\n",
    "    }\n",
    "\n",
    "    # Apply mapping\n",
    "    yearly_data[\"Cloud type\"] = yearly_data[\"Cloud type\"].map(cloud_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing 2021_tracking/01_01.yaml\n",
      "Skipping all clouds file: np -6 to 0\n",
      "Skipping all clouds file: sp -6 to 0\n",
      "Skipping all clouds file: np -12 to -6\n",
      "Skipping all clouds file: sp -12 to -6\n",
      "Skipping all clouds file: np -18 to -12\n",
      "Skipping all clouds file: sp -18 to -12\n",
      "Skipping all clouds file: np -24 to -18\n",
      "Skipping all clouds file: sp -24 to -18\n",
      "Skipping all clouds file: np -30 to -24\n",
      "Skipping all clouds file: sp -30 to -24\n",
      "Skipping all clouds file: np -36 to -30\n",
      "Skipping all clouds file: sp -36 to -30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m config_fp \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m_tracking/\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mpart\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.yaml\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     10\u001b[0m temp_config \u001b[39m=\u001b[39m read_config(config_fp)\n\u001b[0;32m---> 11\u001b[0m df \u001b[39m=\u001b[39m get_combined_cloud_df(temp_config)\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         analysis_df_list\u001b[39m.\u001b[39mappend(df)\n",
      "Cell \u001b[0;32mIn[3], line 66\u001b[0m, in \u001b[0;36mget_combined_cloud_df\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cloud_properties_df_list)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mconcat(\n\u001b[1;32m     67\u001b[0m     [df \u001b[39mfor\u001b[39;49;00m sublist \u001b[39min\u001b[39;49;00m cloud_properties_df_list \u001b[39mfor\u001b[39;49;00m df \u001b[39min\u001b[39;49;00m sublist], ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/cluster/work/climate/dnikolo/flex_trkr_2/lib/python3.11/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/cluster/work/climate/dnikolo/flex_trkr_2/lib/python3.11/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[39m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/cluster/work/climate/dnikolo/flex_trkr_2/lib/python3.11/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "year=2021\n",
    "# def combine_whole_year(year,working_months):\n",
    "analysis_df_list = []\n",
    "glaciation_df_list = []\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for month in months:\n",
    "    for part in range(1,3):\n",
    "        print(f\"Analysing {year}_tracking/{month:02}_{part:02}.yaml\")\n",
    "        config_fp = f'/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/{year}_tracking/{month:02}_{part:02}.yaml'\n",
    "        temp_config = read_config(config_fp)\n",
    "        df = get_combined_cloud_df(temp_config)\n",
    "        if df is not None:\n",
    "                analysis_df_list.append(df)\n",
    "        else:\n",
    "                print(f\"Skiping month {month}\")\n",
    "        glaciation_df_list.append(get_glaciations_df(temp_config))\n",
    "yearly_data = pd.concat(\n",
    "        [df for df in analysis_df_list], ignore_index=True)\n",
    "glaciations_data = pd.concat(\n",
    "        [df for df in glaciation_df_list], ignore_index=True)\n",
    "clasify_clouds(yearly_data)\n",
    "clasify_clouds(glaciations_data)\n",
    "\n",
    "\n",
    "\n",
    "yearly_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_all.parquet\") \n",
    "glaciations_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_glac.parquet\") \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
