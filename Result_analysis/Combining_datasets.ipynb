{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import  partial\n",
    "parent_dir = os.path.dirname(os.environ[\"GTE_DIR\"].replace(\"Glaciation_time_estimator\",\"\"))\n",
    "GTE_DIR=os.environ[\"GTE_DIR\"]\n",
    "sys.path.insert(0, parent_dir)\n",
    "from Glaciation_time_estimator.Data_postprocessing.Job_result_fp_generator import generate_tracking_filenames\n",
    "from Glaciation_time_estimator.Auxiliary_func.config_reader import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(\n",
    "    '/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/config_half.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_array_from_df(series: pd.Series):\n",
    "    if series.empty:\n",
    "        return None\n",
    "    return np.stack(series.values)\n",
    "\n",
    "def get_glaciations_df(config):\n",
    "    agg_fact = config['agg_fact']\n",
    "    folder_name = f\"{config['start_time'].strftime(config['time_folder_format'])}_{config['end_time'].strftime(config['time_folder_format'])}\"\n",
    "    pole=config[\"pole_folders\"][0]\n",
    "    fp = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                pole,\n",
    "                folder_name,\n",
    "                f\"Agg_{agg_fact:02}_Glaciations.parquet\"\n",
    "            )\n",
    "    try:\n",
    "        return pd.read_parquet(fp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping glaciations\")\n",
    "        return \n",
    "\n",
    "def get_combined_cloud_df(config):\n",
    "    t_deltas = config['t_deltas']\n",
    "    agg_fact = config['agg_fact']\n",
    "    min_temp_array, max_temp_array = config['min_temp_arr'], config['max_temp_arr']\n",
    "    folder_name = f\"{config['start_time'].strftime(config['time_folder_format'])}_{config['end_time'].strftime(config['time_folder_format'])}\"\n",
    "    # Initialize an empty list to store the individual dataframes\n",
    "    cloud_properties_df_list = []\n",
    "\n",
    "    # Iterate over each temperature range\n",
    "    for i in range(len(min_temp_array)):\n",
    "        cloud_properties_df_list.append([])\n",
    "        min_temp = min_temp_array[i]\n",
    "        max_temp = max_temp_array[i]\n",
    "\n",
    "        # Iterate over each pole\n",
    "        for pole in config[\"pole_folders\"]:\n",
    "            # Construct the file path\n",
    "            fp = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                pole,\n",
    "                folder_name,\n",
    "                f\"Agg_{agg_fact:02}_T_{abs(round(min_temp)):02}_{abs(round(max_temp)):02}.parquet\"\n",
    "            )\n",
    "\n",
    "            # Read the parquet file into a dataframe\n",
    "            try:\n",
    "                df = pd.read_parquet(fp)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Skipping all clouds file: {pole} {min_temp} to {max_temp}\")\n",
    "                continue\n",
    "\n",
    "            # Add columns for min_temp, max_temp, and pole\n",
    "            df['min_temp'] = min_temp\n",
    "            df['max_temp'] = max_temp\n",
    "            df['pole'] = pole\n",
    "            df['Hemisphere'] = \"South\" if pole == \"sp\" else \"North\"\n",
    "            df['Lifetime [h]'] = df['track_length'] / pd.Timedelta(hours=1)\n",
    "            df[\"Radius [km]\"]=np.sqrt(df[\"avg_size[km]\"]/np.pi)\n",
    "            # Append the dataframe to the sublist\n",
    "            cloud_properties_df_list[i].append(df)\n",
    "\n",
    "    # Combine all dataframes into a single dataframe\n",
    "    if len(cloud_properties_df_list)==0:\n",
    "        return None\n",
    "    return pd.concat(\n",
    "        [df for sublist in cloud_properties_df_list for df in sublist], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasify_clouds(yearly_data):\n",
    "    yearly_data[\"Level\"] = pd.cut(\n",
    "        yearly_data.avg_ctp,\n",
    "        bins=[50, 440, 680, 1000],\n",
    "        labels=[\"Cirro\",\"Alto\",\"Low\"]\n",
    "    )\n",
    "    yearly_data[\"Optical Thickness\"] = pd.cut(\n",
    "        yearly_data.avg_cot,\n",
    "        bins=[0, 3.6, 23, 379],\n",
    "        labels=[\"Thin\", \"Medium\", \"Thick\"]\n",
    "    )\n",
    "\n",
    "    yearly_data[\"Cloud type\"] = list(zip(yearly_data[\"Level\"],yearly_data[\"Optical Thickness\"]))\n",
    "    # Define mapping dictionary\n",
    "    cloud_type_mapping = {\n",
    "        (\"Low\", \"Thin\"): \"Cumulus\",\n",
    "        (\"Alto\", \"Thin\"): \"Altocumulus\",\n",
    "        (\"Cirro\", \"Thin\"): \"Cirrus\",\n",
    "        (\"Low\", \"Medium\"): \"Stratocumulus\",\n",
    "        (\"Alto\", \"Medium\"): \"Altostratus\",\n",
    "        (\"Cirro\", \"Medium\"): \"Cirrostratus\",\n",
    "        (\"Low\", \"Thick\"): \"Stratus\",\n",
    "        (\"Alto\", \"Thick\"): \"Nimbostratus\",\n",
    "        (\"Cirro\", \"Thick\"): \"Deep convection\",\n",
    "    }\n",
    "\n",
    "    # Apply mapping\n",
    "    yearly_data[\"Cloud type\"] = yearly_data[\"Cloud type\"].map(cloud_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'DJF'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'MAM'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'JJA'\n",
    "    else:\n",
    "        return 'SON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing 2022_tracking/01_01.yaml\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'read_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAnalysing \u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m_tracking/\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mpart\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.yaml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m config_fp \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m_tracking/\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mpart\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.yaml\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m temp_config \u001b[39m=\u001b[39m read_config(config_fp)\n\u001b[1;32m     11\u001b[0m df \u001b[39m=\u001b[39m get_combined_cloud_df(temp_config)\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_config' is not defined"
     ]
    }
   ],
   "source": [
    "year=2022\n",
    "# def combine_whole_year(year,working_months):\n",
    "analysis_df_list = []\n",
    "glaciation_df_list = []\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "for month in months:\n",
    "    for part in range(1,3):\n",
    "        print(f\"Analysing {year}_tracking/{month:02}_{part:02}.yaml\")\n",
    "        config_fp = f'/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/{year}_tracking/{month:02}_{part:02}.yaml'\n",
    "        temp_config = read_config(config_fp)\n",
    "        df = get_combined_cloud_df(temp_config)\n",
    "        if df is not None:\n",
    "                analysis_df_list.append(df)\n",
    "        else:\n",
    "                print(f\"Skiping month {month}\")\n",
    "        glaciation_df_list.append(get_glaciations_df(temp_config))\n",
    "yearly_data = pd.concat(\n",
    "        [df for df in analysis_df_list], ignore_index=True)\n",
    "glaciations_data = pd.concat(\n",
    "        [df for df in glaciation_df_list], ignore_index=True)\n",
    "clasify_clouds(yearly_data)\n",
    "clasify_clouds(glaciations_data)\n",
    "yearly_data['Season'] = yearly_data['track_start_time'].dt.month.apply(month_to_season)\n",
    "glaciations_data['Season'] = glaciations_data['track_start_time'].dt.month.apply(month_to_season)\n",
    "\n",
    "yearly_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_all.parquet\") \n",
    "glaciations_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_glac_0.3_thresh.parquet\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only glaciations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year=2022\n",
    "# # def combine_whole_year(year,working_months):\n",
    "# analysis_df_list = []\n",
    "# glaciation_df_list = []\n",
    "# months=[1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "# for month in months:\n",
    "#     for part in range(1,3):\n",
    "#         print(f\"Analysing {year}_tracking/{month:02}_{part:02}.yaml\")\n",
    "#         config_fp = f'/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/{year}_tracking/{month:02}_{part:02}.yaml'\n",
    "#         temp_config = read_config(config_fp)\n",
    "#         df = get_combined_cloud_df(temp_config)\n",
    "#         if df is not None:\n",
    "#                 analysis_df_list.append(df)\n",
    "#         else:\n",
    "#                 print(f\"Skiping month {month}\")\n",
    "#         glaciation_df_list.append(get_glaciations_df(temp_config))\n",
    "# yearly_data = pd.concat(\n",
    "#         [df for df in analysis_df_list], ignore_index=True)\n",
    "# glaciations_data = pd.concat(\n",
    "#         [df for df in glaciation_df_list], ignore_index=True)\n",
    "# clasify_clouds(yearly_data)\n",
    "# clasify_clouds(glaciations_data)\n",
    "# yearly_data['Season'] = yearly_data['track_start_time'].dt.month.apply(month_to_season)\n",
    "# glaciations_data['Season'] = glaciations_data['track_start_time'].dt.month.apply(month_to_season)\n",
    "\n",
    "# yearly_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_all.parquet\") \n",
    "# # glaciations_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_glac.parquet\") a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
