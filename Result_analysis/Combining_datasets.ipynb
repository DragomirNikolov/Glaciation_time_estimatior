{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import  partial\n",
    "sys.path.insert(0, '/cluster/work/climate/dnikolo/n2o')\n",
    "from Glaciation_time_estimator.Data_postprocessing.Job_result_fp_generator import generate_tracking_filenames\n",
    "from Glaciation_time_estimator.Auxiliary_func.config_reader import read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config(\n",
    "    '/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/config_half.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_array_from_df(series: pd.Series):\n",
    "    if series.empty:\n",
    "        return None\n",
    "    return np.stack(series.values)\n",
    "\n",
    "def get_glaciations_df(config):\n",
    "    agg_fact = config['agg_fact']\n",
    "    folder_name = f\"{config['start_time'].strftime(config['time_folder_format'])}_{config['end_time'].strftime(config['time_folder_format'])}\"\n",
    "    pole=config[\"pole_folders\"][0]\n",
    "    fp = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                pole,\n",
    "                folder_name,\n",
    "                f\"Agg_{agg_fact:02}_Glaciations.parquet\"\n",
    "            )\n",
    "    try:\n",
    "        return pd.read_parquet(fp)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping glaciations\")\n",
    "        return \n",
    "\n",
    "def get_combined_cloud_df(config):\n",
    "    t_deltas = config['t_deltas']\n",
    "    agg_fact = config['agg_fact']\n",
    "    min_temp_array, max_temp_array = config['min_temp_arr'], config['max_temp_arr']\n",
    "    folder_name = f\"{config['start_time'].strftime(config['time_folder_format'])}_{config['end_time'].strftime(config['time_folder_format'])}\"\n",
    "    # Initialize an empty list to store the individual dataframes\n",
    "    cloud_properties_df_list = []\n",
    "\n",
    "    # Iterate over each temperature range\n",
    "    for i in range(len(min_temp_array)):\n",
    "        cloud_properties_df_list.append([])\n",
    "        min_temp = min_temp_array[i]\n",
    "        max_temp = max_temp_array[i]\n",
    "\n",
    "        # Iterate over each pole\n",
    "        for pole in config[\"pole_folders\"]:\n",
    "            # Construct the file path\n",
    "            fp = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                pole,\n",
    "                folder_name,\n",
    "                f\"Agg_{agg_fact:02}_T_{abs(round(min_temp)):02}_{abs(round(max_temp)):02}.parquet\"\n",
    "            )\n",
    "\n",
    "            # Read the parquet file into a dataframe\n",
    "            try:\n",
    "                df = pd.read_parquet(fp)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Skipping all clouds file: {pole} {min_temp} to {max_temp}\")\n",
    "                continue\n",
    "\n",
    "            # Add columns for min_temp, max_temp, and pole\n",
    "            df['min_temp'] = min_temp\n",
    "            df['max_temp'] = max_temp\n",
    "            df['pole'] = pole\n",
    "            df['Hemisphere'] = \"South\" if pole == \"sp\" else \"North\"\n",
    "            df['Lifetime [h]'] = df['track_length'] / pd.Timedelta(hours=1)\n",
    "            df[\"Radius [km]\"]=np.sqrt(df[\"avg_size[km]\"]/np.pi)\n",
    "            # Append the dataframe to the sublist\n",
    "            cloud_properties_df_list[i].append(df)\n",
    "\n",
    "    # Combine all dataframes into a single dataframe\n",
    "    return pd.concat(\n",
    "        [df for sublist in cloud_properties_df_list for df in sublist], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasify_clouds(yearly_data):\n",
    "    yearly_data[\"Level\"] = pd.cut(\n",
    "        yearly_data.avg_ctp,\n",
    "        bins=[50, 440, 680, 1000],\n",
    "        labels=[\"Cirro\",\"Alto\",\"Low\"]\n",
    "    )\n",
    "    yearly_data[\"Optical Thickness\"] = pd.cut(\n",
    "        yearly_data.avg_cot,\n",
    "        bins=[0, 3.6, 23, 379],\n",
    "        labels=[\"Thin\", \"Medium\", \"Thick\"]\n",
    "    )\n",
    "\n",
    "    yearly_data[\"Cloud type\"] = list(zip(yearly_data[\"Level\"],yearly_data[\"Optical Thickness\"]))\n",
    "    # Define mapping dictionary\n",
    "    cloud_type_mapping = {\n",
    "        (\"Low\", \"Thin\"): \"Cumulus\",\n",
    "        (\"Alto\", \"Thin\"): \"Altocumulus\",\n",
    "        (\"Cirro\", \"Thin\"): \"Cirrus\",\n",
    "        (\"Low\", \"Medium\"): \"Stratocumulus\",\n",
    "        (\"Alto\", \"Medium\"): \"Altostratus\",\n",
    "        (\"Cirro\", \"Medium\"): \"Cirrostratus\",\n",
    "        (\"Low\", \"Thick\"): \"Stratus\",\n",
    "        (\"Alto\", \"Thick\"): \"Nimbostratus\",\n",
    "        (\"Cirro\", \"Thick\"): \"Deep convection\",\n",
    "    }\n",
    "\n",
    "    # Apply mapping\n",
    "    yearly_data[\"Cloud type\"] = yearly_data[\"Cloud type\"].map(cloud_type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing 2022_tracking/01_01.yaml\n",
      "Analysing 2022_tracking/01_02.yaml\n",
      "Skipping all clouds file: np -18 to -12\n",
      "Skipping all clouds file: sp -36 to -30\n",
      "Analysing 2022_tracking/02_01.yaml\n",
      "Analysing 2022_tracking/02_02.yaml\n",
      "Analysing 2022_tracking/03_01.yaml\n",
      "Analysing 2022_tracking/03_02.yaml\n",
      "Analysing 2022_tracking/04_01.yaml\n",
      "Analysing 2022_tracking/04_02.yaml\n",
      "Skipping all clouds file: np -36 to -30\n",
      "Skipping all clouds file: sp -36 to -30\n",
      "Analysing 2022_tracking/05_01.yaml\n",
      "Skipping all clouds file: sp -6 to 0\n",
      "Skipping all clouds file: sp -12 to -6\n",
      "Skipping all clouds file: sp -18 to -12\n",
      "Skipping all clouds file: np -36 to -30\n",
      "Analysing 2022_tracking/05_02.yaml\n",
      "Analysing 2022_tracking/06_01.yaml\n",
      "Analysing 2022_tracking/06_02.yaml\n",
      "Analysing 2022_tracking/07_01.yaml\n",
      "Analysing 2022_tracking/07_02.yaml\n",
      "Analysing 2022_tracking/08_01.yaml\n",
      "Analysing 2022_tracking/08_02.yaml\n",
      "Analysing 2022_tracking/09_01.yaml\n",
      "Analysing 2022_tracking/09_02.yaml\n",
      "Analysing 2022_tracking/10_01.yaml\n",
      "Analysing 2022_tracking/10_02.yaml\n",
      "Analysing 2022_tracking/11_01.yaml\n",
      "Analysing 2022_tracking/11_02.yaml\n"
     ]
    }
   ],
   "source": [
    "year=2022\n",
    "# def combine_whole_year(year,working_months):\n",
    "analysis_df_list = []\n",
    "glaciation_df_list = []\n",
    "months=[1,2,3,4,5,6,7,8,9,10,11]\n",
    "for month in months:\n",
    "    for part in range(1,3):\n",
    "        print(f\"Analysing {year}_tracking/{month:02}_{part:02}.yaml\")\n",
    "        temp_config = read_config(config_fp)\n",
    "        config_fp = f'/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/{year}_tracking/{month:02}_{part:02}.yaml'\n",
    "        analysis_df_list.append(get_combined_cloud_df(temp_config))\n",
    "        glaciation_df_list.append(get_glaciations_df(temp_config))\n",
    "yearly_data = pd.concat(\n",
    "        [df for df in analysis_df_list], ignore_index=True)\n",
    "glaciations_data = pd.concat(\n",
    "        [df for df in glaciation_df_list], ignore_index=True)\n",
    "clasify_clouds(yearly_data)\n",
    "clasify_clouds(glaciations_data)\n",
    "\n",
    "\n",
    "\n",
    "yearly_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_all.parquet\") \n",
    "glaciations_data.to_parquet(f\"/cluster/work/climate/dnikolo/Cloud_analysis/full_years/{year}_glac.parquet\") \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flex_trkr_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
