{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from Helper_fun import time_intersection, generate_temp_range\n",
    "import os\n",
    "import datetime as dt\n",
    "import math\n",
    "import numba as nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_mode=False\n",
    "Write_csv=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ['WORK_DIR']!='':\n",
    "    WORK_DIR=os.environ['WORK_DIR']\n",
    "    PyFLEXTRKR_LIB_DIR= os.environ['PyFLEXTRKR_LIB_DIR']\n",
    "else:\n",
    "    raise ValueError(\"PyFLEXTRKR_LIB_DIR environmental variable is empty or doesn't exist\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_output_dir=WORK_DIR+'/Job_output'#'/Old_results_storage/Run12.11_all_t_ranges'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cloud:\n",
    "    # def __new__(self, *args, **kwargs):\n",
    "    #     return super().__new__(self)\n",
    "    def __init__(self,cloud_id):\n",
    "        self.id=cloud_id\n",
    "        self.crit_fraction=0.1\n",
    "        # Bools inidicating if the cloud has been liquid at any point\n",
    "        self.is_liq: bool =False\n",
    "        self.is_mix: bool =False\n",
    "        self.is_ice: bool =False\n",
    "        # Max and min cloud size in pixels\n",
    "        self.max_size_km: float =0.0\n",
    "        self.max_size_px: int = 0\n",
    "        self.min_size_km: float =510.0e6\n",
    "        self.min_size_px: int = 3717*3717\n",
    "        \n",
    "        #Variables giving the first and last 4 timesteps (1 hour) of the cloud ice fraction - both arrays run in the same time direction start: [1 , 2 , 3 , 4] ... end: [1 , 2 , 3 , 4]\n",
    "        self.start_ice_fraction_arr=np.empty(4)\n",
    "        self.end_ice_fraction_arr=np.empty(4)\n",
    "        # self.ice_fraction_arr=np.empty(max_timesteps)\n",
    "        self.ice_fraction_list=[]\n",
    "\n",
    "        self.max_water_fraction:float=0.0\n",
    "        self.max_ice_fraction:float=0.0\n",
    "\n",
    "        self.track_start_time: dt.datetime=None\n",
    "        self.track_end_time: dt.datetime=None\n",
    "        self.track_length = None\n",
    "\n",
    "        self.glaciation_start_time: dt.datetime=None\n",
    "        self.glaciation_end_time: dt.datetime=None\n",
    "\n",
    "        self.n_timesteps=None\n",
    "\n",
    "        self.sum_cloud_lat=0.0\n",
    "        self.sum_cloud_lon=0.0\n",
    "        self.avg_cloud_lat=None\n",
    "        self.avg_cloud_lon=None\n",
    "\n",
    "        self.sum_cloud_size_km=0.0\n",
    "        self.avg_cloud_size_km=None\n",
    "\n",
    "        self.sum_cloud_size_px=0.0\n",
    "        self.avg_cloud_size_px=None\n",
    "\n",
    "        self.n_timesteps_no_cloud=0\n",
    "        self.terminate_cloud=False\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.is_liq},{self.is_mix},{self.is_ice},\"\n",
    "\n",
    "    def update_status(self,time: dt.datetime, cloud_values: np.array,cloud_lat,cloud_lon,lat_resolution,lon_resolution):\n",
    "        cloud_size_px=cloud_values.shape[0]\n",
    "        if cloud_size_px:\n",
    "            self.n_timesteps_no_cloud=0\n",
    "            water_fraction=float(np.count_nonzero(cloud_values==1))/float(cloud_size_px)\n",
    "            # ice_fraction=float(np.count_nonzero(cloud_values==2))/float(cloud_size_px)\n",
    "            ice_fraction=1-water_fraction\n",
    "            # assert math.isclose(water_fraction+ice_fraction,1)\n",
    "            #print(water_fraction)\n",
    "            #print(water_fraction)\n",
    "            if not (self.track_start_time):\n",
    "                self.track_start_time=time\n",
    "                self.n_timesteps=1\n",
    "            else:\n",
    "                self.n_timesteps+=1\n",
    "            if self.n_timesteps<=4:\n",
    "                self.start_ice_fraction_arr[self.n_timesteps-1]=ice_fraction\n",
    "            #Check and set type of cloud\n",
    "            if water_fraction>1-self.crit_fraction:\n",
    "                self.is_liq=True\n",
    "            elif water_fraction>self.crit_fraction:\n",
    "                self.is_mix=True\n",
    "            else:\n",
    "                self.is_ice=True\n",
    "\n",
    "            cloud_size_km=lat_resolution*lon_resolution*cloud_size_px*np.cos(np.deg2rad(cloud_lat))*111.321*111.111\n",
    "\n",
    "            self.max_size_km=max(self.max_size_km, cloud_size_km)\n",
    "            self.min_size_km=min(self.min_size_km, cloud_size_km)\n",
    "\n",
    "            self.max_size_px=max(self.max_size_px, cloud_size_px)\n",
    "            self.min_size_px=min(self.min_size_px, cloud_size_px)\n",
    "\n",
    "            self.sum_cloud_size_px+=cloud_size_px\n",
    "            self.avg_cloud_size_px=self.sum_cloud_size_px/self.n_timesteps\n",
    "            \n",
    "            self.sum_cloud_size_km+=cloud_size_km\n",
    "            self.avg_cloud_size_km=self.sum_cloud_size_km/self.n_timesteps\n",
    "            \n",
    "\n",
    "            # I assume that water_frac+ice_frac=1\n",
    "            \n",
    "            self.max_water_fraction=max(self.max_water_fraction, water_fraction)\n",
    "            self.max_ice_fraction=max(self.max_ice_fraction, 1-water_fraction)\n",
    "\n",
    "            self.sum_cloud_lat+=cloud_lat\n",
    "            self.sum_cloud_lon+=cloud_lon\n",
    "            self.avg_cloud_lat=self.sum_cloud_lat/self.n_timesteps\n",
    "            self.avg_cloud_lon=self.sum_cloud_lon/self.n_timesteps\n",
    "\n",
    "            self.track_end_time=time\n",
    "            self.track_length=self.track_end_time-self.track_start_time\n",
    "            \n",
    "            self.end_ice_fraction_arr[0:3]=self.end_ice_fraction_arr[1:4]\n",
    "            self.end_ice_fraction_arr[3]=ice_fraction\n",
    "            \n",
    "            # self.ice_fraction_arr[n_timesteps]=ice_fraction\n",
    "            self.ice_fraction_list.append(ice_fraction)\n",
    "            \n",
    "            \n",
    "    def update_missing_cloud(self):\n",
    "        if self.track_end_time and (not self.terminate_cloud):\n",
    "            self.n_timesteps_no_cloud+=1\n",
    "            if self.n_timesteps_no_cloud > 1:\n",
    "                self.terminate_cloud=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def extract_cloud_coordinates(cloudtracknumber_field,cloud_id_in_field, max_size):\n",
    "    # Define the dictionary with the appropriate types\n",
    "    loc_hash_map_cloud_numbers={j: (0,np.zeros((2,max_size),dtype=np.int16)) for j in cloud_id_in_field}\n",
    "    # # Traverse the 3D array\n",
    "    # for i in cloud_id_in_field:\n",
    "    #     loc_hash_map_cloud_numbers[val] = (0,np.empty((2,max_size),dtype=np.int16))\n",
    "    for row in range(cloudtracknumber_field.shape[1]):\n",
    "        for col in range(cloudtracknumber_field.shape[2]):\n",
    "            val = cloudtracknumber_field[0, row, col]\n",
    "            if val !=0:\n",
    "                ind,cord=loc_hash_map_cloud_numbers[val]\n",
    "                if ind<=max_size:\n",
    "                    cord[:,ind]=np.asarray([row,col],dtype=np.int16)\n",
    "                    ind+=1\n",
    "                    # print(ind)\n",
    "                    loc_hash_map_cloud_numbers[val] = (ind,cord)\n",
    "    return loc_hash_map_cloud_numbers\n",
    "    # return loc_hash_map_cloud_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2,5,10,15,38\n",
    "t_deltas = [5]\n",
    "min_temp_array, max_temp_array = generate_temp_range(t_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_fact_list=[3]#[1,2,5,10]#[2,3,5,10]#1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing T: -5 to 0 Agg=3\n",
      "-5 to 0 Loading 20040201_141500\n",
      "-5 to 0 Loading 20040201_143000\n",
      "-5 to 0 Loading 20040201_144500\n",
      "-5 to 0 Loading 20040201_150000\n",
      "-5 to 0 Loading 20040201_151500\n",
      "-5 to 0 Loading 20040201_153000\n",
      "-5 to 0 Loading 20040201_154500\n",
      "-5 to 0 Loading 20040201_160000\n",
      "-5 to 0 Loading 20040201_161500\n",
      "-5 to 0 Loading 20040201_163000\n",
      "-5 to 0 Loading 20040201_164500\n",
      "-5 to 0 Loading 20040201_170000\n",
      "-5 to 0 Loading 20040201_171500\n",
      "-5 to 0 Loading 20040201_173000\n",
      "-5 to 0 Loading 20040201_174500\n",
      "-5 to 0 Loading 20040201_180000\n",
      "-5 to 0 Loading 20040201_181500\n",
      "-5 to 0 Loading 20040201_183000\n",
      "-5 to 0 Loading 20040201_184500\n",
      "-5 to 0 Loading 20040201_190000\n",
      "-5 to 0 Loading 20040201_191500\n",
      "-5 to 0 Loading 20040201_193000\n",
      "-5 to 0 Loading 20040201_194500\n",
      "-5 to 0 Loading 20040201_200000\n",
      "Analyzing T: -10 to -5 Agg=3\n",
      "-10 to -5 Loading 20040201_141500\n",
      "-10 to -5 Loading 20040201_143000\n",
      "-10 to -5 Loading 20040201_144500\n",
      "-10 to -5 Loading 20040201_150000\n",
      "-10 to -5 Loading 20040201_151500\n",
      "-10 to -5 Loading 20040201_153000\n",
      "-10 to -5 Loading 20040201_154500\n",
      "-10 to -5 Loading 20040201_160000\n",
      "-10 to -5 Loading 20040201_161500\n",
      "-10 to -5 Loading 20040201_163000\n",
      "-10 to -5 Loading 20040201_164500\n",
      "-10 to -5 Loading 20040201_170000\n",
      "-10 to -5 Loading 20040201_171500\n",
      "-10 to -5 Loading 20040201_173000\n",
      "-10 to -5 Loading 20040201_174500\n",
      "-10 to -5 Loading 20040201_180000\n",
      "-10 to -5 Loading 20040201_181500\n",
      "-10 to -5 Loading 20040201_183000\n",
      "-10 to -5 Loading 20040201_184500\n",
      "-10 to -5 Loading 20040201_190000\n",
      "-10 to -5 Loading 20040201_191500\n",
      "-10 to -5 Loading 20040201_193000\n",
      "-10 to -5 Loading 20040201_194500\n",
      "-10 to -5 Loading 20040201_200000\n",
      "Analyzing T: -15 to -10 Agg=3\n",
      "-15 to -10 Loading 20040201_141500\n",
      "-15 to -10 Loading 20040201_143000\n",
      "-15 to -10 Loading 20040201_144500\n",
      "-15 to -10 Loading 20040201_150000\n",
      "-15 to -10 Loading 20040201_151500\n",
      "-15 to -10 Loading 20040201_153000\n",
      "-15 to -10 Loading 20040201_154500\n",
      "-15 to -10 Loading 20040201_160000\n",
      "-15 to -10 Loading 20040201_161500\n",
      "-15 to -10 Loading 20040201_163000\n",
      "-15 to -10 Loading 20040201_164500\n",
      "-15 to -10 Loading 20040201_170000\n",
      "-15 to -10 Loading 20040201_171500\n",
      "-15 to -10 Loading 20040201_173000\n",
      "-15 to -10 Loading 20040201_174500\n",
      "-15 to -10 Loading 20040201_180000\n",
      "-15 to -10 Loading 20040201_181500\n",
      "-15 to -10 Loading 20040201_183000\n",
      "-15 to -10 Loading 20040201_184500\n",
      "-15 to -10 Loading 20040201_190000\n",
      "-15 to -10 Loading 20040201_191500\n",
      "-15 to -10 Loading 20040201_193000\n",
      "-15 to -10 Loading 20040201_194500\n",
      "-15 to -10 Loading 20040201_200000\n",
      "Analyzing T: -20 to -15 Agg=3\n",
      "-20 to -15 Loading 20040201_141500\n",
      "-20 to -15 Loading 20040201_143000\n",
      "-20 to -15 Loading 20040201_144500\n",
      "-20 to -15 Loading 20040201_150000\n",
      "-20 to -15 Loading 20040201_151500\n",
      "-20 to -15 Loading 20040201_153000\n",
      "-20 to -15 Loading 20040201_154500\n",
      "-20 to -15 Loading 20040201_160000\n",
      "-20 to -15 Loading 20040201_161500\n",
      "-20 to -15 Loading 20040201_163000\n",
      "-20 to -15 Loading 20040201_164500\n",
      "-20 to -15 Loading 20040201_170000\n",
      "-20 to -15 Loading 20040201_171500\n",
      "-20 to -15 Loading 20040201_173000\n",
      "-20 to -15 Loading 20040201_174500\n",
      "-20 to -15 Loading 20040201_180000\n",
      "-20 to -15 Loading 20040201_181500\n",
      "-20 to -15 Loading 20040201_183000\n",
      "-20 to -15 Loading 20040201_184500\n",
      "-20 to -15 Loading 20040201_190000\n",
      "-20 to -15 Loading 20040201_191500\n",
      "-20 to -15 Loading 20040201_193000\n",
      "-20 to -15 Loading 20040201_194500\n",
      "-20 to -15 Loading 20040201_200000\n",
      "Analyzing T: -25 to -20 Agg=3\n",
      "-25 to -20 Loading 20040201_141500\n",
      "-25 to -20 Loading 20040201_143000\n",
      "-25 to -20 Loading 20040201_144500\n",
      "-25 to -20 Loading 20040201_150000\n",
      "-25 to -20 Loading 20040201_151500\n",
      "-25 to -20 Loading 20040201_153000\n",
      "-25 to -20 Loading 20040201_154500\n",
      "-25 to -20 Loading 20040201_160000\n",
      "-25 to -20 Loading 20040201_161500\n",
      "-25 to -20 Loading 20040201_163000\n",
      "-25 to -20 Loading 20040201_164500\n",
      "-25 to -20 Loading 20040201_170000\n",
      "-25 to -20 Loading 20040201_171500\n",
      "-25 to -20 Loading 20040201_173000\n",
      "-25 to -20 Loading 20040201_174500\n",
      "-25 to -20 Loading 20040201_180000\n",
      "-25 to -20 Loading 20040201_181500\n",
      "-25 to -20 Loading 20040201_183000\n",
      "-25 to -20 Loading 20040201_184500\n",
      "-25 to -20 Loading 20040201_190000\n",
      "-25 to -20 Loading 20040201_191500\n",
      "-25 to -20 Loading 20040201_193000\n",
      "-25 to -20 Loading 20040201_194500\n",
      "-25 to -20 Loading 20040201_200000\n",
      "Analyzing T: -30 to -25 Agg=3\n",
      "-30 to -25 Loading 20040201_141500\n",
      "-30 to -25 Loading 20040201_143000\n",
      "-30 to -25 Loading 20040201_144500\n",
      "-30 to -25 Loading 20040201_150000\n",
      "-30 to -25 Loading 20040201_151500\n",
      "-30 to -25 Loading 20040201_153000\n",
      "-30 to -25 Loading 20040201_154500\n",
      "-30 to -25 Loading 20040201_160000\n",
      "-30 to -25 Loading 20040201_161500\n",
      "-30 to -25 Loading 20040201_163000\n",
      "-30 to -25 Loading 20040201_164500\n",
      "-30 to -25 Loading 20040201_170000\n",
      "-30 to -25 Loading 20040201_171500\n",
      "-30 to -25 Loading 20040201_173000\n",
      "-30 to -25 Loading 20040201_174500\n",
      "-30 to -25 Loading 20040201_180000\n",
      "-30 to -25 Loading 20040201_181500\n",
      "-30 to -25 Loading 20040201_183000\n",
      "-30 to -25 Loading 20040201_184500\n",
      "-30 to -25 Loading 20040201_190000\n",
      "-30 to -25 Loading 20040201_191500\n",
      "-30 to -25 Loading 20040201_193000\n",
      "-30 to -25 Loading 20040201_194500\n",
      "-30 to -25 Loading 20040201_200000\n",
      "Analyzing T: -35 to -30 Agg=3\n",
      "-35 to -30 Loading 20040201_141500\n",
      "-35 to -30 Loading 20040201_143000\n",
      "-35 to -30 Loading 20040201_144500\n",
      "-35 to -30 Loading 20040201_150000\n",
      "-35 to -30 Loading 20040201_151500\n",
      "-35 to -30 Loading 20040201_153000\n",
      "-35 to -30 Loading 20040201_154500\n",
      "-35 to -30 Loading 20040201_160000\n",
      "-35 to -30 Loading 20040201_161500\n",
      "-35 to -30 Loading 20040201_163000\n",
      "-35 to -30 Loading 20040201_164500\n",
      "-35 to -30 Loading 20040201_170000\n",
      "-35 to -30 Loading 20040201_171500\n",
      "-35 to -30 Loading 20040201_173000\n",
      "-35 to -30 Loading 20040201_174500\n",
      "-35 to -30 Loading 20040201_180000\n",
      "-35 to -30 Loading 20040201_181500\n",
      "-35 to -30 Loading 20040201_183000\n",
      "-35 to -30 Loading 20040201_184500\n",
      "-35 to -30 Loading 20040201_190000\n",
      "-35 to -30 Loading 20040201_191500\n",
      "-35 to -30 Loading 20040201_193000\n",
      "-35 to -30 Loading 20040201_194500\n",
      "-35 to -30 Loading 20040201_200000\n"
     ]
    }
   ],
   "source": [
    "cloud_list_agg=[]\n",
    "for agg_fact in agg_fact_list:\n",
    "    cloud_list=[]\n",
    "    sum_append_cloud=dt.timedelta(seconds=0)\n",
    "    sum_analyze_cloud=dt.timedelta(seconds=0)\n",
    "    sum_load_and_analyze_cloud=dt.timedelta(seconds=0)\n",
    "    sum_load_track_variables=dt.timedelta(seconds=0)\n",
    "    sum_data_loading=dt.timedelta(seconds=0)\n",
    "    sum_current_cloud_load=dt.timedelta(seconds=0)\n",
    "    sum_update_status_time=dt.timedelta(seconds=0)\n",
    "    for temp_ind in range(len(min_temp_array)):\n",
    "        # loop_start_time=dt.datetime.now()\n",
    "        loop_start_time=dt.datetime.now()\n",
    "        min_temp, max_temp = min_temp_array[temp_ind],max_temp_array[temp_ind]\n",
    "        #Load datasets\n",
    "        try:\n",
    "            if agg_fact!=0:\n",
    "                current_iteration_dir= job_output_dir+f'/T-{abs(round(min_temp))}-{abs(round(max_temp))}-agg-{agg_fact}'\n",
    "            else:\n",
    "                current_iteration_dir= job_output_dir+f'/T-{abs(round(min_temp))}-{abs(round(max_temp))}/'\n",
    "            cloudtrack_data = nc.Dataset(current_iteration_dir+'/pixel_path_tracking/20040201.1415_20040201.2000/cloudtracks_20040201_183000.nc')\n",
    "            trackstats_data=nc.Dataset(current_iteration_dir+'/stats/trackstats_final_20040201.1415_20040201.2000.nc')\n",
    "            tracknumbers_data=nc.Dataset(current_iteration_dir+'/stats/tracknumbers_20040201.1415_20040201.2000.nc')\n",
    "        except: #Exception as inst:\n",
    "            print(f\"Skipping {min_temp} to {max_temp}\")\n",
    "            # print(type(inst))    # the exception type\n",
    "\n",
    "            # print(inst.args)     # arguments stored in .args\n",
    "\n",
    "            # print(inst)          # __str__ allows args to be printed directly,\n",
    "\n",
    "                                # but may be overridden in exception subclasses\n",
    "            cloud_list.append([])\n",
    "            continue\n",
    "        #Load relevant data from datasets into local variables\n",
    "        cloudtracknumber_field=cloudtrack_data.variables['tracknumber'][:,:,:]\n",
    "        cph_field=cloudtrack_data.variables['cph'][:,:,:]\n",
    "        n_tracks=trackstats_data.variables['track_duration'].shape[0]\n",
    "        basetimes=tracknumbers_data.variables['basetimes'][:]\n",
    "        lat=cloudtrack_data.variables['lat'][:]\n",
    "        lon=cloudtrack_data.variables['lon'][:]\n",
    "        lat_resolution=(lat.max()-lat.min())/len(lat)\n",
    "        lon_resolution=(lon.max()-lon.min())/len(lon)\n",
    "        trackstats_data.close()\n",
    "        tracknumbers_data.close()\n",
    "        cloudtrack_data.close()\n",
    "        #FIX CLOUD TIMES\n",
    "        # print(n_tracks)\n",
    "        # append_start_time=dt.datetime.now()\n",
    "        append_start_time=dt.datetime.now()\n",
    "        sum_load_track_variables+=append_start_time-loop_start_time\n",
    "        # print(append_start_time-loop_start_time)\n",
    "        cloud_list.append([cloud(f'{temp_ind}_{i}') for i in range(n_tracks)])\n",
    "        append_end_time=dt.datetime.now()\n",
    "        sum_append_cloud+=append_end_time-append_start_time\n",
    "        # print(append_end_time-append_start_time)\n",
    "        print(f'Analyzing T: {min_temp} to {max_temp} Agg={agg_fact}')\n",
    "        for unix_time in basetimes:\n",
    "            data_loading_start_time=dt.datetime.now()\n",
    "            time=dt.datetime.utcfromtimestamp(unix_time)\n",
    "            time_str=time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            print(f'{min_temp} to {max_temp} Loading {time_str}')\n",
    "            cloudtrack_fp = current_iteration_dir+f'/pixel_path_tracking/20040201.1415_20040201.2000/cloudtracks_{time_str}.nc'\n",
    "            cloudtrack_data = nc.Dataset(cloudtrack_fp) \n",
    "            cloudtracknumber_field=cloudtrack_data.variables['tracknumber'][:,:,:].data\n",
    "            cph_field=cloudtrack_data.variables['cph'][:,:,:]\n",
    "            cloudtrack_data.close()\n",
    "            analysis_start_time=dt.datetime.now()\n",
    "            sum_data_loading+=analysis_start_time-data_loading_start_time\n",
    "            cloud_id_in_field , counts = np.unique(cloudtracknumber_field, return_counts=True)\n",
    "            counts=counts[cloud_id_in_field!=0]\n",
    "            cloud_id_in_field=cloud_id_in_field[cloud_id_in_field!=0]\n",
    "            max_allowed_cloud_size_px= 20000 if fast_mode else counts.max()\n",
    "            hash_map_cloud_numbers=extract_cloud_coordinates(cloudtracknumber_field,cloud_id_in_field,max_allowed_cloud_size_px)#counts.max())\n",
    "            if max_allowed_cloud_size_px>1000000:\n",
    "                print(np.where(counts, counts==counts.max()))\n",
    "            for track_number in cloud_id_in_field:\n",
    "                current_cloud_select_time=dt.datetime.now()\n",
    "                try:\n",
    "                    current_cloud=cloud_list[temp_ind][track_number-1]\n",
    "                except:\n",
    "                    print(f\"Error: {temp_ind,track_number,len(cloud_list[temp_ind])}\")\n",
    "                    continue\n",
    "                start_update_status_time=dt.datetime.now()\n",
    "                if (not current_cloud.terminate_cloud):\n",
    "                    sum_current_cloud_load+=start_update_status_time-current_cloud_select_time\n",
    "                    #TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "                    ind ,cord=hash_map_cloud_numbers[track_number]\n",
    "                    cloud_location_ind=[cord[0,:ind],cord[1,:ind]]\n",
    "                    if cloud_location_ind[0].size!=0:\n",
    "                        avg_lat_ind=int(round(np.mean(cloud_location_ind[0])))\n",
    "                        avg_lon_ind=int(round(np.mean(cloud_location_ind[1])))\n",
    "                        #TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "                        cloud_values=cph_field[0,cloud_location_ind[0].T,cloud_location_ind[1].T]\n",
    "                        current_cloud.update_status(time,cloud_values,lat[avg_lat_ind],lon[avg_lon_ind],lat_resolution,lon_resolution)\n",
    "                    else:\n",
    "                        current_cloud.update_missing_cloud()\n",
    "                sum_update_status_time+=dt.datetime.now()-start_update_status_time\n",
    "            sum_analyze_cloud+=dt.datetime.now()-analysis_start_time\n",
    "            \n",
    "            # cloudtrack_data.close()\n",
    "        sum_load_and_analyze_cloud+=dt.datetime.now()-append_end_time\n",
    "        # print(dt.datetime.now()-append_end_time)\n",
    "\n",
    "    cloud_list_agg.append(cloud_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.011519\n",
      "0:01:14.329384\n",
      "0:01:32.467609\n",
      "0:00:01.410100\n",
      "0:00:18.137019\n",
      "0:00:00.023006\n",
      "0:00:01.698441\n"
     ]
    }
   ],
   "source": [
    "print(sum_append_cloud)\n",
    "print(sum_analyze_cloud)\n",
    "print(sum_load_and_analyze_cloud)\n",
    "print(sum_load_track_variables)\n",
    "print(sum_data_loading)\n",
    "print(sum_current_cloud_load)\n",
    "print(sum_update_status_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_5_0_agg_3_tracknumber\n",
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_10_5_agg_3_tracknumber\n",
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_15_10_agg_3_tracknumber\n",
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_20_15_agg_3_tracknumber\n",
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_25_20_agg_3_tracknumber\n",
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_30_25_agg_3_tracknumber\n",
      "Writing to  /cluster/work/climate/dnikolo//Cloud_analysis/T_35_30_agg_3_tracknumber\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "lat_resolution=(lat.max()-lat.min())/len(lat)\n",
    "lon_resolution=(lon.max()-lon.min())/len(lon)\n",
    "columns=[\"is_liq\",\"is_mix\",\"is_ice\",\"max_water_frac\",\"max_ice_fraction\",\"avg_size[km]\",\"max_size[km]\",\"min_size[km]\",\"avg_size[px]\",\"max_size[px]\",\"min_size[px]\",\"track_start_time\",\"track_length\",\"glaciation_start_time\",\"glaciation_end_time\",\"avg_lat\",\"avg_lon\",\"start_ice_fraction\",\"end_ice_fraction\",\"ice_frac_hist\"]\n",
    "datapoints_per_cloud=len(columns)\n",
    "for agg_ind in range(len(agg_fact_list)):\n",
    "    cloud_list = cloud_list_agg[agg_ind]\n",
    "    agg_fact=agg_fact_list[agg_ind]\n",
    "    for dt_ind in range(len(t_deltas)):\n",
    "        # TODO Change iteration method in for loop so that for each dt it only goes through the relevant parts of cloud list instead of the whole\n",
    "        for cloud_list_ind in range(len(cloud_list)):\n",
    "            temp_delta=t_deltas[dt_ind]\n",
    "            cloudinfo_df=pd.DataFrame(index=range(len(cloud_list[cloud_list_ind])),columns=columns)\n",
    "            for cloud_ind in range(len(cloud_list[cloud_list_ind])):\n",
    "                current_cloud=cloud_list[cloud_list_ind][cloud_ind]\n",
    "                # current_cloud.max_size_km=current_cloud.max_size_px * lat_resolution*lon_resolution*np.cos(np.deg2rad(current_cloud.avg_cloud_lat))*111.321*111.111\n",
    "                # current_cloud.min_size_km=current_cloud.min_size_px * lat_resolution*lon_resolution*np.cos(np.deg2rad(current_cloud.avg_cloud_lat))*111.321*111.111\n",
    "\n",
    "                cloudinfo_df.iloc[cloud_ind]=[current_cloud.is_liq,current_cloud.is_mix,current_cloud.is_ice,current_cloud.max_water_fraction,current_cloud.max_ice_fraction,current_cloud.avg_cloud_size_km,current_cloud.max_size_km,current_cloud.min_size_km,current_cloud.avg_cloud_size_px ,current_cloud.max_size_px,current_cloud.min_size_px,current_cloud.track_start_time,current_cloud.track_length,current_cloud.glaciation_start_time , current_cloud.glaciation_end_time, current_cloud.avg_cloud_lat, current_cloud.avg_cloud_lon, current_cloud.start_ice_fraction_arr, current_cloud.end_ice_fraction_arr, current_cloud.ice_fraction_list]\n",
    "            min_temp, max_temp = min_temp_array[cloud_list_ind],max_temp_array[cloud_list_ind]\n",
    "\n",
    "            output_dir=WORK_DIR+f\"/Cloud_analysis/T_{abs(round(min_temp))}_{abs(round(max_temp))}_agg_{agg_fact}_tracknumber\"\n",
    "            print(\"Writing to \",output_dir)\n",
    "            output_dir_parq=output_dir+\".parquet\"\n",
    "            cloudinfo_df.to_parquet(output_dir_parq)\n",
    "            if Write_csv:\n",
    "                output_dir_csv=output_dir+\".csv\"\n",
    "                cloudinfo_df.to_csv(output_dir_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import datetime as dt\n",
    "# lat_resolution=(lat.max()-lat.min())/len(lat)\n",
    "# lon_resolution=(lon.max()-lon.min())/len(lon)\n",
    "# columns=[\"is_liq\",\"is_mix\",\"is_ice\",\"max_water_frac\",\"max_ice_fraction\",\"avg_size[km]\",\"max_size[km]\",\"min_size[km]\",\"max_size[px]\",\"min_size[px]\",\"track_start_time\",\"track_length\",\"glaciation_start_time\",\"glaciation_end_time\",\"avg_lat\",\"avg_lon\", \"start_ice_fraction\",\"end_ice_fraction\"]\n",
    "# datapoints_per_cloud=len(columns)\n",
    "# for agg_ind in range(len(agg_fact_list)):\n",
    "#     cloud_list = cloud_list_agg[agg_ind]\n",
    "#     agg_fact=agg_fact_list[agg_ind]\n",
    "#     for dt_ind in range(len(t_deltas)):\n",
    "#         # TODO Change iteration method in for loop so that for each dt it only goes through the relevant parts of cloud list instead of the whole\n",
    "#         for cloud_list_ind in range(len(cloud_list)):\n",
    "#             temp_delta=t_deltas[dt_ind]\n",
    "#             cloudinfo_df=pd.DataFrame(index=range(len(cloud_list[cloud_list_ind])),columns=columns)\n",
    "#             for cloud_ind in range(len(cloud_list[cloud_list_ind])):\n",
    "#                 current_cloud=cloud_list[cloud_list_ind][cloud_ind]\n",
    "#                 # current_cloud.max_size_km=current_cloud.max_size_px * lat_resolution*lon_resolution*np.cos(np.deg2rad(current_cloud.avg_cloud_lat))*111.321*111.111\n",
    "#                 # current_cloud.min_size_km=current_cloud.min_size_px * lat_resolution*lon_resolution*np.cos(np.deg2rad(current_cloud.avg_cloud_lat))*111.321*111.111\n",
    "\n",
    "#                 cloudinfo_df.iloc[cloud_ind]=[current_cloud.is_liq,current_cloud.is_mix,current_cloud.is_ice,current_cloud.max_water_fraction,current_cloud.max_ice_fraction,current_cloud.avg_cloud_size_km,current_cloud.max_size_km,current_cloud.min_size_km,current_cloud.max_size_px,current_cloud.min_size_px,current_cloud.track_start_time,current_cloud.track_length,current_cloud.glaciation_start_time , current_cloud.glaciation_end_time, current_cloud.avg_cloud_lat, current_cloud.avg_cloud_lon, current_cloud.start_ice_fraction_arr, current_cloud.end_ice_fraction_arr]\n",
    "#             min_temp, max_temp = min_temp_array[cloud_list_ind],max_temp_array[cloud_list_ind]\n",
    "\n",
    "#             output_dir=WORK_DIR+f\"/Cloud_analysis/T_{abs(round(min_temp))}_{abs(round(max_temp))}_agg_{agg_fact}_tracknumber\"\n",
    "#             output_dir_csv=output_dir+\".csv\"\n",
    "#             output_dir_parq=output_dir+\".parquet\"\n",
    "#             print(\"Writing to \",output_dir_csv)\n",
    "#             # cloudinfo_df.to_csv(output_dir_csv)\n",
    "#             cloudinfo_xr=cloudinfo_df.to_parquet(output_dir_parq)\n",
    "#             #  cloudinfo_xr.to_netcdf(output_dir_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloudinfo_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n_track_arr=np.empty(len(min_temp_array))\n",
    "# n_glac_arr=np.empty(len(min_temp_array))\n",
    "\n",
    "# for i in range(len(cloud_list)):\n",
    "#     n_track_arr[i]=len(cloud_list[i])\n",
    "\n",
    "#     glaciation_counter=0\n",
    "#     only_ice_counter=0\n",
    "#     only_mix_counter=0\n",
    "#     only_liq_counter=0\n",
    "#     cirrus_counter=0\n",
    "#     for cloud in cloud_list[i]:\n",
    "#         print(f\"Cloud max ice fraction: {round(cloud.max_ice_fraction,2)}; Cloud max liq fraction: {round(cloud.max_water_fraction,2)} \" )\n",
    "#         if cloud.is_liq+cloud.is_mix+cloud.is_ice==3:\n",
    "#             glaciation_counter+=1\n",
    "#         if (not (cloud.is_liq or cloud.is_mix) ) and cloud.is_ice:\n",
    "#             cirrus_counter+=1\n",
    "#         if (not (cloud.is_ice or cloud.is_mix) ) and cloud.is_liq:\n",
    "#             only_liq_counter+=1\n",
    "#         if (not (cloud.is_liq or cloud.is_ice) ) and cloud.is_mix:\n",
    "#             only_mix_counter+=1\n",
    "#         if cloud.is_ice:\n",
    "#             only_ice_counter+=1\n",
    "#     n_glac_arr[i]=only_mix_counter# glaciations\n",
    "        \n",
    "# print(f\"N glaciations: {glaciation_counter}; N only cirrus: {cirrus_counter}; N only liquid: {only_liq_counter}; N only mix: {only_mix_counter}; N ice at some point: {only_ice_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Function to generate time strings\n",
    "# def iterate_time(start_date, end_date):\n",
    "#     current_time = start_date\n",
    "#     while current_time <= end_date:\n",
    "#         # Format the time as YYYYMODD_HHMMSS\n",
    "#         time_str = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "#         print(time_str)\n",
    "#         # Increment the time by 15 minutes\n",
    "#         current_time += timedelta(minutes=15)\n",
    "\n",
    "# # Example usage\n",
    "# start_date = datetime(2024, 10, 15, 9, 0, 0)  # Example start date: 15th Oct 2024, 9:00:00\n",
    "# end_date = datetime(2024, 10, 15, 12, 0, 0)    # Example end date: 15th Oct 2024, 12:00:00\n",
    "\n",
    "# iterate_time(start_date, end_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # Create labels for the x-axis (temperature ranges)\n",
    "\n",
    "\n",
    "# labels = [f\"{min_temp_array[i]} to {max_temp_array[i]}\" for i in range(len(min_temp_array))]\n",
    "\n",
    "# # Create the bar graph\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.bar(labels, n_track_arr[np.where(max_temp_array-min_temp_array==5)], color='skyblue',label=\"N tracks\")\n",
    "# plt.bar(labels, n_glac_arr[np.where(max_temp_array-min_temp_array==5)], color='lightcoral',label=\"N glaciations\")\n",
    "\n",
    "# # Add title and labels\n",
    "# plt.title('Number Distribution Across Temperature Ranges', fontsize=14, fontweight='bold')\n",
    "# plt.xlabel('Temperature Range (Â°C)', fontsize=12)\n",
    "# plt.ylabel('Cloud track number', fontsize=12)\n",
    "\n",
    "# # Rotate x-axis labels for better readability\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# # Add gridlines for better visualization\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(f'/net/n2o/wolke_scratch/dnikolo/Glaciation_time_estimatior/Result_graphs/TEST_track_distribution_dt5.png',dpi=400)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud_list_agg=[]\n",
    "# for agg_fact in agg_fact_list:\n",
    "#     cloud_list=[]\n",
    "#     sum_append_cloud=dt.timedelta(seconds=0)\n",
    "#     sum_analyze_cloud=dt.timedelta(seconds=0)\n",
    "#     sum_load_and_analyze_cloud=dt.timedelta(seconds=0)\n",
    "#     sum_load_track_variables=dt.timedelta(seconds=0)\n",
    "#     sum_data_loading=dt.timedelta(seconds=0)\n",
    "#     sum_current_cloud_load=dt.timedelta(seconds=0)\n",
    "#     sum_update_status_time=dt.timedelta(seconds=0)\n",
    "#     for temp_ind in range(len(min_temp_array)):\n",
    "#         # loop_start_time=dt.datetime.now()\n",
    "#         loop_start_time=dt.datetime.now()\n",
    "#         min_temp, max_temp = min_temp_array[temp_ind],max_temp_array[temp_ind]\n",
    "#         #Load datasets\n",
    "#         try:\n",
    "#             if agg_fact!=0:\n",
    "#                 current_iteration_dir= job_output_dir+f'/T-{abs(round(min_temp))}-{abs(round(max_temp))}-agg-{agg_fact}/'\n",
    "#             else:\n",
    "#                 current_iteration_dir= job_output_dir+f'/T-{abs(round(min_temp))}-{abs(round(max_temp))}/'\n",
    "#             cloudtrack_data = nc.Dataset(current_iteration_dir+'/pixel_path_tracking/20040201.1415_20040201.2000/cloudtracks_20040201_183000.nc')\n",
    "#             trackstats_data=nc.Dataset(current_iteration_dir+'/stats/trackstats_final_20040201.1415_20040201.2000.nc')\n",
    "#             tracknumbers_data=nc.Dataset(current_iteration_dir+'/stats/tracknumbers_20040201.1415_20040201.2000.nc')\n",
    "#         except: #Exception as inst:\n",
    "#             print(f\"Skipping {min_temp} to {max_temp}\")\n",
    "#             # print(type(inst))    # the exception type\n",
    "\n",
    "#             # print(inst.args)     # arguments stored in .args\n",
    "\n",
    "#             # print(inst)          # __str__ allows args to be printed directly,\n",
    "\n",
    "#                                 # but may be overridden in exception subclasses\n",
    "#             cloud_list.append([])\n",
    "#             continue\n",
    "#         #Load relevant data from datasets into local variables\n",
    "#         cloudtracknumber_field=cloudtrack_data.variables['tracknumber'][:,:,:]\n",
    "#         cph_field=cloudtrack_data.variables['cph'][:,:,:]\n",
    "#         n_tracks=trackstats_data.variables['track_duration'].shape[0]\n",
    "#         basetimes=tracknumbers_data.variables['basetimes'][:]\n",
    "#         lat=cloudtrack_data.variables['lat'][:]\n",
    "#         lon=cloudtrack_data.variables['lon'][:]\n",
    "#         lat_resolution=(lat.max()-lat.min())/len(lat)\n",
    "#         lon_resolution=(lon.max()-lon.min())/len(lon)\n",
    "#         trackstats_data.close()\n",
    "#         tracknumbers_data.close()\n",
    "#         cloudtrack_data.close()\n",
    "#         #FIX CLOUD TIMES\n",
    "#         print(n_tracks)\n",
    "#         # append_start_time=dt.datetime.now()\n",
    "#         append_start_time=dt.datetime.now()\n",
    "#         sum_load_track_variables+=append_start_time-loop_start_time\n",
    "#         print(append_start_time-loop_start_time)\n",
    "#         cloud_list.append([cloud(f'{temp_ind}_{i}') for i in range(n_tracks)])\n",
    "#         append_end_time=dt.datetime.now()\n",
    "#         sum_append_cloud+=append_end_time-append_start_time\n",
    "#         print(append_end_time-append_start_time)\n",
    "#         for unix_time in basetimes:\n",
    "#             data_loading_start_time=dt.datetime.now()\n",
    "#             time=dt.datetime.utcfromtimestamp(unix_time)\n",
    "#             time_str=strftime(\"%Y%m%d_%H%M%S\")\n",
    "#             print(f'{min_temp} to {max_temp} Loading {time_str}')\n",
    "#             cloudtrack_fp = current_iteration_dir+f'/pixel_path_tracking/20040201.1415_20040201.2000/cloudtracks_{time_str}.nc'\n",
    "#             cloudtrack_data = nc.Dataset(cloudtrack_fp) \n",
    "#             cloudtracknumber_field=cloudtrack_data.variables['tracknumber'][:,:,:]\n",
    "#             cph_field=cloudtrack_data.variables['cph'][:,:,:]\n",
    "#             cloudtrack_data.close()\n",
    "#             analysis_start_time=dt.datetime.now()\n",
    "#             sum_data_loading+=analysis_start_time-data_loading_start_time\n",
    "#             for track_number in range(n_tracks):\n",
    "#                 current_cloud_select_time=dt.datetime.now()\n",
    "#                 try:\n",
    "#                     current_cloud=cloud_list[temp_ind][track_number]\n",
    "#                 except:\n",
    "#                     print(f\"Error: {temp_ind,track_number,len(cloud_list)}\")\n",
    "#                     exit\n",
    "#                 start_update_status_time=dt.datetime.now()\n",
    "#                 if (not current_cloud.terminate_cloud):\n",
    "#                     sum_current_cloud_load+=start_update_status_time-current_cloud_select_time\n",
    "#                     #TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "#                     cloud_location_ind=np.where(cloudtracknumber_field==track_number+1)\n",
    "#                     if cloud_location_ind[0].size!=0:\n",
    "#                         avg_lat_ind=int(round(np.mean(cloud_location_ind[1])))\n",
    "#                         avg_lon_ind=int(round(np.mean(cloud_location_ind[2])))\n",
    "#                         #TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "#                         cloud_values=cph_field[cloud_location_ind]\n",
    "#                         current_cloud.update_status(time,cloud_values,lat[avg_lat_ind],lon[avg_lon_ind],lat_resolution,lon_resolution)\n",
    "#                     else:\n",
    "#                         current_cloud.update_missing_cloud()\n",
    "#                 sum_update_status_time+=dt.datetime.now()-start_update_status_time\n",
    "#             sum_analyze_cloud+=dt.datetime.now()-analysis_start_time\n",
    "            \n",
    "#             # cloudtrack_data.close()\n",
    "#         sum_load_and_analyze_cloud+=dt.datetime.now()-append_end_time\n",
    "#         print(dt.datetime.now()-append_end_time)\n",
    "\n",
    "#     cloud_list_agg.append(cloud_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
