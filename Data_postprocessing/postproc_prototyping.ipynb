{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import numba as nb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from Single_cloud_analysis import Cloud\n",
    "import sys\n",
    "sys.path.insert(0,'/cluster/work/climate/dnikolo/n2o')\n",
    "from Glaciation_time_estimator.Auxiliary_func.config_reader import read_config\n",
    "from Glaciation_time_estimator.Data_postprocessing.Job_result_fp_generator import generate_tracking_filenames\n",
    "from multiprocessing import Manager\n",
    "from Glaciation_time_estimator.Auxiliary_func.Nestable_multiprocessing import NestablePool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "class Cloud:\n",
    "    # def __new__(self, *args, **kwargs):\n",
    "    #     return super().__new__(self)\n",
    "    def __init__(self, cloud_id):\n",
    "        self.id = cloud_id\n",
    "        self.crit_fraction = 0.1\n",
    "        # Bools inidicating if the cloud has been liquid at any point\n",
    "        self.is_liq: bool = False\n",
    "        self.is_mix: bool = False\n",
    "        self.is_ice: bool = False\n",
    "        # Max and min cloud size in pixels\n",
    "        self.max_size_km: float = 0.0\n",
    "        self.max_size_px: int = 0\n",
    "        self.min_size_km: float = 510.0e6\n",
    "        self.min_size_px: int = 3717*3717\n",
    "\n",
    "        # Variables giving the first and last 4 timesteps (1 hour) of the cloud ice fraction - both arrays run in the same time direction start: [1 , 2 , 3 , 4] ... end: [1 , 2 , 3 , 4]\n",
    "        self.start_ice_fraction_arr = np.empty(4)\n",
    "        self.end_ice_fraction_arr = np.empty(4)\n",
    "        # self.ice_fraction_arr=np.empty(max_timesteps)\n",
    "        self.ice_fraction_list = []\n",
    "        \n",
    "\n",
    "        self.max_water_fraction: float = 0.0\n",
    "        self.max_ice_fraction: float = 0.0\n",
    "\n",
    "        self.track_start_time: dt.datetime = None\n",
    "        self.track_end_time: dt.datetime = None\n",
    "        self.track_length = None\n",
    "\n",
    "        self.glaciation_start_time: dt.datetime = None\n",
    "        self.glaciation_end_time: dt.datetime = None\n",
    "\n",
    "        self.n_timesteps = None\n",
    "\n",
    "        self.sum_cloud_lat = 0.0\n",
    "        self.sum_cloud_lon = 0.0\n",
    "        self.avg_cloud_lat = None\n",
    "        self.avg_cloud_lon = None\n",
    "        self.lon_list=[]\n",
    "        self.lat_list=[]\n",
    "\n",
    "        self.sum_cloud_size_km = 0.0\n",
    "        self.avg_cloud_size_km = None\n",
    "        self.cloud_size_km_list = []\n",
    "\n",
    "        self.sum_cloud_size_px = 0.0\n",
    "        self.avg_cloud_size_px = None\n",
    "\n",
    "        self.n_timesteps_no_cloud = 0\n",
    "        self.terminate_cloud = False\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.is_liq},{self.is_mix},{self.is_ice},\"\n",
    "\n",
    "    def update_status(self, time: dt.datetime, cloud_values: np.array, cloud_lat, cloud_lon, lat_resolution, lon_resolution):\n",
    "        cloud_size_px = cloud_values.shape[0]\n",
    "        # print(cloud_values)\n",
    "        if cloud_size_px:\n",
    "            self.n_timesteps_no_cloud = 0\n",
    "            valid_values = cloud_values[cloud_values >= 1]\n",
    "            # print(len(valid_values)/len(cloud_values))\n",
    "            ice_fraction = (valid_values.sum() -\n",
    "                            float(len(valid_values)))/float(len(valid_values))\n",
    "            # print(valid_values)\n",
    "            # ice_fraction=float(np.count_nonzero(cloud_values==2))/float(cloud_size_px)\n",
    "            water_fraction = 1-ice_fraction\n",
    "            # assert math.isclose(water_fraction+ice_fraction,1)\n",
    "            # print(water_fraction)\n",
    "            # print(water_fraction)f cloud_arr[track_number-1] is None:\n",
    "            if not (self.track_start_time):\n",
    "                self.track_start_time = time\n",
    "                self.n_timesteps = 1\n",
    "            else:\n",
    "                self.n_timesteps += 1\n",
    "            if self.n_timesteps <= 4:\n",
    "                self.start_ice_fraction_arr[self.n_timesteps-1] = ice_fraction\n",
    "            # Check and set type of cloud\n",
    "            if water_fraction > 1-self.crit_fraction:\n",
    "                self.is_liq = True\n",
    "            elif water_fraction > self.crit_fraction:\n",
    "                self.is_mix = True\n",
    "            else:\n",
    "                self.is_ice = True\n",
    "\n",
    "            cloud_size_km = lat_resolution*lon_resolution*cloud_size_px * \\\n",
    "                np.cos(np.deg2rad(cloud_lat))*111.321*111.111\n",
    "            self.cloud_size_km_list.append(cloud_size_km)\n",
    "            self.max_size_km = max(self.max_size_km, cloud_size_km)\n",
    "            self.min_size_km = min(self.min_size_km, cloud_size_km)\n",
    "\n",
    "            self.max_size_px = max(self.max_size_px, cloud_size_px)\n",
    "            self.min_size_px = min(self.min_size_px, cloud_size_px)\n",
    "\n",
    "            self.sum_cloud_size_px += cloud_size_px\n",
    "            self.avg_cloud_size_px = self.sum_cloud_size_px/self.n_timesteps\n",
    "\n",
    "            self.sum_cloud_size_km += cloud_size_km\n",
    "            self.avg_cloud_size_km = self.sum_cloud_size_km/self.n_timesteps\n",
    "\n",
    "            # I assume that water_frac+ice_frac=1\n",
    "\n",
    "            self.max_water_fraction = max(\n",
    "                self.max_water_fraction, water_fraction)\n",
    "            self.max_ice_fraction = max(\n",
    "                self.max_ice_fraction, 1-water_fraction)\n",
    "\n",
    "            self.sum_cloud_lat += cloud_lat\n",
    "            self.sum_cloud_lon += cloud_lon\n",
    "            self.lon_list.append(cloud_lon)\n",
    "            self.lat_list.append(cloud_lat)\n",
    "            self.avg_cloud_lat = self.sum_cloud_lat/self.n_timesteps\n",
    "            self.avg_cloud_lon = self.sum_cloud_lon/self.n_timesteps\n",
    "\n",
    "            self.track_end_time = time\n",
    "            self.track_length = self.track_end_time-self.track_start_time\n",
    "\n",
    "            self.end_ice_fraction_arr[0:3] = self.end_ice_fraction_arr[1:4]\n",
    "            self.end_ice_fraction_arr[3] = ice_fraction\n",
    "\n",
    "            # self.ice_fraction_arr[n_timesteps]=ice_fraction\n",
    "            self.ice_fraction_list.append(ice_fraction)\n",
    "\n",
    "    def update_missing_cloud(self):\n",
    "        if self.track_end_time and (not self.terminate_cloud):\n",
    "            self.n_timesteps_no_cloud += 1\n",
    "            if self.n_timesteps_no_cloud > 1:\n",
    "                self.terminate_cloud = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def extract_cloud_coordinates(cloudtracknumber_field, cloud_id_in_field, max_size):\n",
    "    # Define the dictionary with the appropriate types\n",
    "    loc_hash_map_cloud_numbers = {\n",
    "        j: (0, np.zeros((2, max_size), dtype=np.int16)) for j in cloud_id_in_field}\n",
    "    # # Traverse the 3D array\n",
    "    # for i in cloud_id_in_field:\n",
    "    #     loc_hash_map_cloud_numbers[val] = (0,np.empty((2,max_size),dtype=np.int16))\n",
    "    for row in range(cloudtracknumber_field.shape[1]):\n",
    "        for col in range(cloudtracknumber_field.shape[2]):\n",
    "            val = cloudtracknumber_field[0, row, col]\n",
    "            if val != 0:\n",
    "                ind, cord = loc_hash_map_cloud_numbers[val]\n",
    "                if ind <= max_size:\n",
    "                    cord[:, ind] = np.asarray([row, col], dtype=np.int16)\n",
    "                    ind += 1\n",
    "                    # print(ind)\n",
    "                    loc_hash_map_cloud_numbers[val] = (ind, cord)\n",
    "    return loc_hash_map_cloud_numbers\n",
    "    # return loc_hash_map_cloud_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_value(val):\n",
    "    if isinstance(val, xr.DataArray):\n",
    "        return val.values.item() if val.size == 1 else val.values\n",
    "    return val\n",
    "\n",
    "def analize_single_temp_range(temp_ind: int, cloud_dict, tracking_fps, pole: str, config: dict) -> None:\n",
    "    # loop_start_time=dt.datetime.now()\n",
    "    min_temp, max_temp = config['min_temp_arr'][temp_ind], config['max_temp_arr'][temp_ind]\n",
    "    # Load datasets\n",
    "    temp_key = f'{abs(round(min_temp))}_{abs(round(max_temp))}'\n",
    "    try:\n",
    "        cloudtrack_data = xr.load_dataset(tracking_fps[pole][temp_key][\"cloudtracks\"][0])\n",
    "        trackstats_data = xr.load_dataset(\n",
    "            tracking_fps[pole][temp_key][\"trackstats_final\"])\n",
    "        tracknumbers_data = xr.load_dataset(tracking_fps[pole][temp_key][\"tracknumbers\"])\n",
    "    except:  # Exception as inst:\n",
    "        print(f\"Skipping {min_temp} to {max_temp}\")\n",
    "        cloud_dict[temp_key] = np.array([])\n",
    "        return None\n",
    "    # Load relevant data from datasets into local variables\n",
    "    n_tracks = trackstats_data.variables['track_duration'].shape[0]\n",
    "    basetimes = pd.to_datetime(tracknumbers_data['basetimes'])\n",
    "    lat = cloudtrack_data['lat']\n",
    "    lon = cloudtrack_data['lon']\n",
    "    lat_resolution = (lat.max()-lat.min())/len(lat)\n",
    "    lon_resolution = (lon.max()-lon.min())/len(lon)\n",
    "    trackstats_data.close()\n",
    "    tracknumbers_data.close()\n",
    "    cloudtrack_data.close()\n",
    "    # print(append_start_time-loop_start_time)\n",
    "    cloud_arr = np.empty((n_tracks), dtype=Cloud)\n",
    "    # Cloud(f'{temp_ind}_{i}') for i in range(n_tracks)])\n",
    "    # print(append_end_time-append_start_time)\n",
    "    print(f\"Analyzing T: {min_temp} to {max_temp} Agg={config['agg_fact']}\")\n",
    "    for fp_ind in range(3):#range(len(basetimes)):\n",
    "        time = basetimes[fp_ind]\n",
    "        time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        print(f'{min_temp} to {max_temp} Loading {time_str}')\n",
    "        cloudtrack_fp = tracking_fps[pole][temp_key]['cloudtracks'][fp_ind]\n",
    "        cloudtrack_data = xr.load_dataset(cloudtrack_fp)\n",
    "        cloudtracknumber_field = cloudtrack_data['tracknumber'].data\n",
    "        cloudtracknumber_field[np.isnan(cloudtracknumber_field)] = 0\n",
    "        cloudtracknumber_field = cloudtracknumber_field.astype(int)\n",
    "        cph_field = cloudtrack_data['cph_filtered']\n",
    "        cloud_id_in_field, counts = np.unique(\n",
    "            cloudtracknumber_field, return_counts=True)\n",
    "        counts = counts[cloud_id_in_field != 0]\n",
    "        cloud_id_in_field = cloud_id_in_field[cloud_id_in_field != 0]\n",
    "        max_allowed_cloud_size_px = config['fast_mode_arr_size'] if config['postprocessing_fast_mode'] else counts.max()\n",
    "        hash_map_cloud_numbers = extract_cloud_coordinates(\n",
    "            cloudtracknumber_field, cloud_id_in_field, max_allowed_cloud_size_px)  # counts.max())\n",
    "        cloudtrack_data.close()\n",
    "        if max_allowed_cloud_size_px > 1000000:\n",
    "            print(np.where(counts, counts == counts.max()))\n",
    "        # print(cloud_id_in_field)\n",
    "        for track_number in cloud_id_in_field:\n",
    "            try:\n",
    "                if cloud_arr[track_number-1] is None:\n",
    "                    cloud_arr[track_number-1] = Cloud(temp_key)\n",
    "            except:\n",
    "                print(\n",
    "                    f\"Error: {temp_ind,track_number,len(cloud_arr)}\")\n",
    "                continue\n",
    "\n",
    "            if (not cloud_arr[track_number-1].terminate_cloud):\n",
    "                # TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "                ind, cord = hash_map_cloud_numbers[track_number]\n",
    "                cloud_location_ind = [cord[0, :ind], cord[1, :ind]]\n",
    "                if cloud_location_ind[0].size != 0:\n",
    "                    avg_lat_ind = int(round(np.mean(cloud_location_ind[0])))\n",
    "                    avg_lon_ind = int(round(np.mean(cloud_location_ind[1])))\n",
    "                    # TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "                    cloud_values = cph_field.values[0,\n",
    "                                             cloud_location_ind[0].T, cloud_location_ind[1].T]\n",
    "                    cloud_arr[track_number-1].update_status(\n",
    "                        time, cloud_values, extract_value(lat[avg_lat_ind]), extract_value(lon[avg_lon_ind]), lat_resolution.values, lon_resolution.values)\n",
    "                else:\n",
    "                    cloud_arr[track_number-1].update_missing_cloud()\n",
    "    cloud_dict[f\"{pole}_{temp_key}\"] = cloud_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = read_config(config_fp=\"/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/config.yaml\")\n",
    "tracking_fps = generate_tracking_filenames(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing T: -5 to 0 Agg=3\n",
      "-5 to 0 Loading 20230108_000000\n",
      "-5 to 0 Loading 20230108_001500\n",
      "-5 to 0 Loading 20230108_003000\n"
     ]
    }
   ],
   "source": [
    "res_dict={}\n",
    "analize_single_temp_range(0,res_dict,tracking_fps, \"np\",config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/20230108.0000_20230115.2345/T_05_00_agg_03.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract values from xarray.DataArray or leave them unchanged\n",
    "\n",
    "\n",
    "min_temp, max_temp = config['min_temp_arr'][0], config['max_temp_arr'][0]\n",
    "temp_key = f'{abs(round(min_temp))}_{abs(round(max_temp))}'\n",
    "cloudtrack_data = xr.load_dataset(\n",
    "    tracking_fps['np'][temp_key][\"cloudtracks\"][0])\n",
    "lat = cloudtrack_data['lat']\n",
    "lon = cloudtrack_data['lon']\n",
    "lat_resolution = extract_value((lat.max()-lat.min())/len(lat))\n",
    "lon_resolution = extract_value((lon.max()-lon.min())/len(lon))\n",
    "cloudtrack_data.close()\n",
    "columns = [\"is_liq\", \"is_mix\", \"is_ice\", \"max_water_frac\",\n",
    "           \"max_ice_fraction\", \"avg_size[km]\", \"max_size[km]\",\n",
    "           \"min_size[km]\", \"avg_size[px]\", \"max_size[px]\",\n",
    "           \"min_size[px]\", \"track_start_time\", \"track_length\",\n",
    "           \"glaciation_start_time\", \"glaciation_end_time\", \"avg_lat\",\n",
    "           \"avg_lon\", \"start_ice_fraction\", \"end_ice_fraction\",\n",
    "           \"ice_frac_hist\", \"lat_hist\", \"lon_hist\", \n",
    "           \"size_hist_km\"]\n",
    "datapoints_per_cloud = len(columns)\n",
    "# Iterating through the cloud data\n",
    "# for temp_ind in range(len(config['max_temp_arr'])):\n",
    "temp_ind = 0\n",
    "# for pole in config['pole_folders']:\n",
    "pole = 'np'\n",
    "min_temp, max_temp = config['min_temp_arr'][temp_ind], config['max_temp_arr'][temp_ind]\n",
    "temp_key = f'{abs(round(min_temp))}_{abs(round(max_temp))}'\n",
    "key = f'{pole}_{temp_key}'\n",
    "cloud_arr = res_dict[key]\n",
    "\n",
    "cloudinfo_df = pd.DataFrame(index=range(len(cloud_arr)), columns=columns)\n",
    "for cloud_ind in range(len(cloud_arr)):\n",
    "    current_cloud = cloud_arr[cloud_ind]\n",
    "    if current_cloud is not None:\n",
    "        cloudinfo_df.iloc[cloud_ind] = [\n",
    "            current_cloud.is_liq,\n",
    "            current_cloud.is_mix,\n",
    "            current_cloud.is_ice,\n",
    "            current_cloud.max_water_fraction,\n",
    "            current_cloud.max_ice_fraction,\n",
    "            extract_value(current_cloud.avg_cloud_size_km),\n",
    "            extract_value(current_cloud.max_size_km),\n",
    "            extract_value(current_cloud.min_size_km),\n",
    "            extract_value(current_cloud.avg_cloud_size_px),\n",
    "            extract_value(current_cloud.max_size_px),\n",
    "            extract_value(current_cloud.min_size_px),\n",
    "            current_cloud.track_start_time,\n",
    "            current_cloud.track_length,\n",
    "            current_cloud.glaciation_start_time,\n",
    "            current_cloud.glaciation_end_time,\n",
    "            extract_value(current_cloud.avg_cloud_lat),\n",
    "            extract_value(current_cloud.avg_cloud_lon),\n",
    "            current_cloud.start_ice_fraction_arr,\n",
    "            current_cloud.end_ice_fraction_arr,\n",
    "            current_cloud.ice_fraction_list,\n",
    "            current_cloud.lat_list,\n",
    "            current_cloud.lon_list,\n",
    "            current_cloud.cloud_size_km_list\n",
    "        ]\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = os.path.join(\n",
    "    config['postprocessing_output_dir'],\n",
    "    config['time_folder_name'],\n",
    "    f\"T_{abs(round(min_temp)):02}_{abs(round(max_temp)):02}_agg_{config['agg_fact']:02}\"\n",
    ")\n",
    "os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
    "\n",
    "# Save DataFrame to Parquet\n",
    "output_dir_parq = output_dir + \".parquet\"\n",
    "print(\"Writing to \", output_dir_parq)\n",
    "cloudinfo_df.to_parquet(output_dir_parq)\n",
    "\n",
    "# Optionally save as CSV\n",
    "if config['write_csv']:\n",
    "    output_dir_csv = output_dir + \".csv\"\n",
    "    cloudinfo_df.to_csv(output_dir_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flex_trkr_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
