{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import numba as nb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import os\n",
    "from multiprocessing import Manager\n",
    "import sys\n",
    "sys.path.insert(0,'/cluster/work/climate/dnikolo/n2o')\n",
    "from Glaciation_time_estimator.Auxiliary_func.config_reader import read_config\n",
    "from Glaciation_time_estimator.Data_postprocessing.Single_cloud_analysis import Cloud\n",
    "from Glaciation_time_estimator.Data_postprocessing.Job_result_fp_generator import generate_tracking_filenames\n",
    "from Glaciation_time_estimator.Auxiliary_func.Nestable_multiprocessing import NestablePool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def extract_cloud_coordinates(cloudtracknumber_field, cloud_id_in_field, max_size):\n",
    "    # Define the dictionary with the appropriate types\n",
    "    loc_hash_map_cloud_numbers = {\n",
    "        j: (0, np.zeros((2, max_size), dtype=np.int16)) for j in cloud_id_in_field}\n",
    "    # # Traverse the 3D array\n",
    "    # for i in cloud_id_in_field:\n",
    "    #     loc_hash_map_cloud_numbers[val] = (0,np.empty((2,max_size),dtype=np.int16))\n",
    "    for row in range(cloudtracknumber_field.shape[1]):\n",
    "        for col in range(cloudtracknumber_field.shape[2]):\n",
    "            val = cloudtracknumber_field[0, row, col]\n",
    "            if val != 0:\n",
    "                ind, cord = loc_hash_map_cloud_numbers[val]\n",
    "                if ind <= max_size:\n",
    "                    cord[:, ind] = np.asarray([row, col], dtype=np.int16)\n",
    "                    ind += 1\n",
    "                    # print(ind)\n",
    "                    loc_hash_map_cloud_numbers[val] = (ind, cord)\n",
    "    return loc_hash_map_cloud_numbers\n",
    "    # return loc_hash_map_cloud_numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoordinateTransformer:\n",
    "    def __init__(self, target_shape, agg_fact):\n",
    "        self.agg_fact = agg_fact\n",
    "        self.target_shape=target_shape\n",
    "\n",
    "    def transform(self, lat_ind, lon_ind):\n",
    "        transformed_lat_ind = np.empty((len(lat_ind)*self.agg_fact**2), dtype=int)\n",
    "        transformed_lon_ind = np.empty((len(lon_ind)*self.agg_fact**2), dtype=int)\n",
    "        step = self.agg_fact**2\n",
    "        for k in range(step):\n",
    "            i=k//self.agg_fact\n",
    "            j=k%self.agg_fact\n",
    "            transformed_lat_ind[k::step] = lat_ind*self.agg_fact+i\n",
    "            transformed_lon_ind[k::step] = lon_ind*self.agg_fact+j\n",
    "        mask = (transformed_lat_ind < self.target_shape[0]) & (transformed_lon_ind < self.target_shape[1]) \n",
    "        # print(mask)\n",
    "        transformed_lon_ind = transformed_lon_ind[mask]\n",
    "        transformed_lat_ind = transformed_lat_ind[mask]\n",
    "        return transformed_lat_ind.T, transformed_lon_ind.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_ds = xr.load_dataset(\"/cluster/work/climate/dnikolo/Auxiliary_files/CM_SAF_CLAAS3_L2_AUX_NP.nc\",decode_times=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = np.arange(0,10,2)\n",
    "# a = np.arange(5)\n",
    "# a\n",
    "# coord_transform = CoordinateTransformer(aux_ds.lon.shape[1:],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(val):\n",
    "    if isinstance(val, xr.DataArray):\n",
    "        return val.values.item() if val.size == 1 else val.values\n",
    "    return val\n",
    "\n",
    "\n",
    "def save_single_temp_range_results(cloud_arr, pole, min_temp, max_temp, config):\n",
    "    columns = [\"is_liq\", \"is_mix\", \"is_ice\", \"max_water_frac\",\n",
    "               \"max_ice_fraction\", \"avg_size[km]\", \"max_size[km]\",\n",
    "               \"min_size[km]\", \"avg_size[px]\", \"max_size[px]\",\n",
    "               \"min_size[px]\", \"track_start_time\", \"track_length\",\n",
    "               \"glaciation_start_time\", \"glaciation_end_time\", \"avg_lat\",\n",
    "               \"avg_lon\", \"start_ice_fraction\", \"end_ice_fraction\",\n",
    "               \"ice_frac_hist\", \"lat_hist\", \"lon_hist\",\n",
    "               \"size_hist_km\"]\n",
    "    datapoints_per_cloud = len(columns)\n",
    "    cloudinfo_df = pd.DataFrame(\n",
    "        index=range(len(cloud_arr)), columns=columns)\n",
    "    for cloud_ind in range(len(cloud_arr)):\n",
    "        current_cloud = cloud_arr[cloud_ind]\n",
    "        if current_cloud is not None:\n",
    "            cloudinfo_df.iloc[cloud_ind] = [\n",
    "                current_cloud.is_liq,\n",
    "                current_cloud.is_mix,\n",
    "                current_cloud.is_ice,\n",
    "                current_cloud.max_water_fraction,\n",
    "                current_cloud.max_ice_fraction,\n",
    "                extract_value(current_cloud.avg_cloud_size_km),\n",
    "                extract_value(current_cloud.max_size_km),\n",
    "                extract_value(current_cloud.min_size_km),\n",
    "                extract_value(current_cloud.avg_cloud_size_px),\n",
    "                extract_value(current_cloud.max_size_px),\n",
    "                extract_value(current_cloud.min_size_px),\n",
    "                current_cloud.track_start_time,\n",
    "                current_cloud.track_length,\n",
    "                current_cloud.glaciation_start_time,\n",
    "                current_cloud.glaciation_end_time,\n",
    "                extract_value(current_cloud.avg_cloud_lat),\n",
    "                extract_value(current_cloud.avg_cloud_lon),\n",
    "                current_cloud.start_ice_fraction_arr,\n",
    "                current_cloud.end_ice_fraction_arr,\n",
    "                current_cloud.ice_fraction_list,\n",
    "                current_cloud.lat_list,\n",
    "                current_cloud.lon_list,\n",
    "                current_cloud.cloud_size_km_list\n",
    "            ]\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.join(\n",
    "        config['postprocessing_output_dir'], pole,\n",
    "        config['time_folder_name'],\n",
    "        f\"Agg_{config['agg_fact']:02}_T_{abs(round(min_temp)):02}_{abs(round(max_temp)):02}\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
    "\n",
    "    # Save DataFrame to Parquet\n",
    "    output_dir_parq = output_dir + \".parquet\"\n",
    "    print(\"Writing to \", output_dir_parq)\n",
    "    cloudinfo_df.to_parquet(output_dir_parq)\n",
    "\n",
    "    # Optionally save as CSV\n",
    "    if config['write_csv']:\n",
    "        output_dir_csv = output_dir + \".csv\"\n",
    "        cloudinfo_df.to_csv(output_dir_csv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analize_single_temp_range(temp_ind: int, cloud_dict, tracking_fps: dict, pole: str, config: dict, pix_area=None,  lon=None, lat=None) -> None:\n",
    "    # loop_start_time=dt.datetime.now()\n",
    "    min_temp, max_temp = config['min_temp_arr'][temp_ind], config['max_temp_arr'][temp_ind]\n",
    "    is_resampled = config[\"Resample\"]\n",
    "    # Load datasets\n",
    "    temp_key = f'{abs(round(min_temp))}_{abs(round(max_temp))}'\n",
    "    print(f\"Analyzing {pole} {temp_key}\")\n",
    "    # print(tracking_fps[pole][temp_key][\"cloudtracks\"][0])\n",
    "    # print(tracking_fps[pole][temp_key][\"trackstats_final\"])\n",
    "    # print(tracking_fps[pole][temp_key][\"tracknumbers\"])\n",
    "    try:\n",
    "        cloudtrack_data = xr.load_dataset(\n",
    "            tracking_fps[pole][temp_key][\"cloudtracks\"][0])\n",
    "        trackstats_data = xr.load_dataset(\n",
    "            tracking_fps[pole][temp_key][\"trackstats_final\"])\n",
    "        tracknumbers_data = xr.load_dataset(\n",
    "            tracking_fps[pole][temp_key][\"tracknumbers\"])\n",
    "    except:  # Exception as inst:\n",
    "        print(f\"Skipping {pole} {min_temp} to {max_temp}\")\n",
    "        cloud_dict[temp_key] = np.array([])\n",
    "        return None\n",
    "    # Load relevant data from datasets into local variables\n",
    "    n_tracks = trackstats_data.variables['track_duration'].shape[0]\n",
    "    basetimes = pd.to_datetime(tracknumbers_data['basetimes'])\n",
    "    if is_resampled:\n",
    "        lat = cloudtrack_data['lat']\n",
    "        lon = cloudtrack_data['lon']\n",
    "        lat_resolution = (lat.max()-lat.min())/len(lat)\n",
    "        lon_resolution = (lon.max()-lon.min())/len(lon)\n",
    "    else:\n",
    "        coord_transformer = CoordinateTransformer(lon.shape[1:],config[\"agg_fact\"])\n",
    "    trackstats_data.close()\n",
    "    tracknumbers_data.close()\n",
    "    cloudtrack_data.close()\n",
    "    # print(append_start_time-loop_start_time)\n",
    "    cloud_arr = np.empty((n_tracks), dtype=Cloud)\n",
    "    # Cloud(f'{temp_ind}_{i}') for i in range(n_tracks)])\n",
    "    # print(append_end_time-append_start_time)\n",
    "    # print(f\"Analyzing T: {min_temp} to {max_temp} Agg={config['agg_fact']}\")\n",
    "    for fp_ind in range(len(basetimes)):\n",
    "        time = basetimes[fp_ind]\n",
    "        # time_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # print(f'{min_temp} to {max_temp} Loading {time_str}')\n",
    "        cloudtrack_fp = tracking_fps[pole][temp_key]['cloudtracks'][fp_ind]\n",
    "        cloudtrack_data = xr.load_dataset(cloudtrack_fp)\n",
    "        cloudtracknumber_field = cloudtrack_data['tracknumber'].data\n",
    "        cloudtracknumber_field[np.isnan(cloudtracknumber_field)] = 0\n",
    "        cloudtracknumber_field = cloudtracknumber_field.astype(int)\n",
    "        cph_field = cloudtrack_data['cph_filtered']\n",
    "        cloud_id_in_field, counts = np.unique(\n",
    "            cloudtracknumber_field, return_counts=True)\n",
    "        counts = counts[cloud_id_in_field != 0]\n",
    "        if len(counts) == 0:\n",
    "            continue\n",
    "        cloud_id_in_field = cloud_id_in_field[cloud_id_in_field != 0]\n",
    "        max_allowed_cloud_size_px = config['fast_mode_arr_size'] if config['postprocessing_fast_mode'] else counts.max(\n",
    "        )\n",
    "        hash_map_cloud_numbers = extract_cloud_coordinates(\n",
    "            cloudtracknumber_field, cloud_id_in_field, max_allowed_cloud_size_px)  # counts.max())\n",
    "        cloudtrack_data.close()\n",
    "        if max_allowed_cloud_size_px > 1000000:\n",
    "            print(np.where(counts, counts == counts.max()))\n",
    "        # print(cloud_id_in_field)\n",
    "        for track_number in cloud_id_in_field:\n",
    "            try:\n",
    "                if cloud_arr[track_number-1] is None:\n",
    "                    cloud_arr[track_number-1] = Cloud(temp_key, is_resampled)\n",
    "            except:\n",
    "                print(\n",
    "                    f\"Error: {temp_ind,track_number,len(cloud_arr)}\")\n",
    "                continue\n",
    "\n",
    "            if (not cloud_arr[track_number-1].terminate_cloud):\n",
    "                # TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "                ind, cord = hash_map_cloud_numbers[track_number]\n",
    "                cloud_location_ind = [cord[0, :ind], cord[1, :ind]]\n",
    "                if cloud_location_ind[0].size != 0:\n",
    "                    cloud_cph_values = cph_field.values[0,\n",
    "                                                        cloud_location_ind[0].T, cloud_location_ind[1].T]\n",
    "                    if is_resampled:\n",
    "                        avg_lat_ind = int(\n",
    "                            round(np.mean(cloud_location_ind[0])))\n",
    "                        avg_lon_ind = int(\n",
    "                            round(np.mean(cloud_location_ind[1])))\n",
    "                        # TODO:SPEED UP NEXT TWO LINES (set_cloud_values and update_status)\n",
    "                        cloud_arr[track_number-1].update_status(\n",
    "                            time, cloud_cph_values, extract_value(lat[avg_lat_ind]), extract_value(lon[avg_lon_ind]), pixel_area=lat_resolution.values*lon_resolution.values)\n",
    "                    else:\n",
    "                        cloud_location_ind_non_agg = coord_transformer.transform(\n",
    "                            cloud_location_ind[0], cloud_location_ind[1])\n",
    "                        cloud_cph_values = cph_field.values[0,\n",
    "                                                            cloud_location_ind[0].T, cloud_location_ind[1].T]\n",
    "                        cloud_pix_area_values = pix_area.values[0,\n",
    "                                                                cloud_location_ind_non_agg[0], cloud_location_ind_non_agg[1]]\n",
    "                        cloud_lat_values = lat.values[0,\n",
    "                                                      cloud_location_ind_non_agg[0], cloud_location_ind_non_agg[1]]\n",
    "                        cloud_lon_values = lon.values[0,\n",
    "                                                      cloud_location_ind_non_agg[0], cloud_location_ind_non_agg[1]]\n",
    "                        cloud_arr[track_number-1].update_status(\n",
    "                            time, cloud_cph_values, cloud_lat_values, cloud_lon_values, cloud_pix_area_values)\n",
    "                else:\n",
    "                    cloud_arr[track_number-1].update_missing_cloud()\n",
    "    save_single_temp_range_results(cloud_arr, pole, min_temp, max_temp, config)\n",
    "\n",
    "\n",
    "def analize_single_pole(pole, cloud_dict, tracking_fps, config, n_procs=1):\n",
    "    print(f\"Analyzing {pole}\")\n",
    "    aux_ds = xr.load_dataset(config[\"aux_fps_eu\"][pole],decode_times=False)\n",
    "    if config[\"Resample\"]:\n",
    "        with NestablePool(n_procs) as pool:\n",
    "            part_single_temp_range = partial(\n",
    "                analize_single_temp_range, cloud_dict=cloud_dict, tracking_fps=tracking_fps, pole=pole, config=config)\n",
    "            pool.map(part_single_temp_range, range(\n",
    "                len(config['min_temp_arr'])))\n",
    "            pool.close()\n",
    "    if not config[\"Resample\"]:\n",
    "        lat_mat = aux_ds[\"lat\"].load()\n",
    "        lon_mat = aux_ds[\"lon\"].load()\n",
    "        pix_area = aux_ds[\"pixel_area\"].load()\n",
    "        with NestablePool(n_procs) as pool:\n",
    "            part_single_temp_range = partial(\n",
    "                analize_single_temp_range, cloud_dict=cloud_dict, tracking_fps=tracking_fps, pole=pole, config=config, pix_area=pix_area, lon=lon_mat, lat=lat_mat)\n",
    "            pool.map(part_single_temp_range, range(\n",
    "                len(config['min_temp_arr'])))\n",
    "            pool.close()\n",
    "\n",
    "\n",
    "def save_results(res_dict, config):\n",
    "    min_temp, max_temp = config['min_temp_arr'][0], config['max_temp_arr'][0]\n",
    "    temp_key = f'{abs(round(min_temp))}_{abs(round(max_temp))}'\n",
    "    # cloudtrack_data = xr.(\n",
    "    #     tracking_fps['np'][temp_key][\"cloudtracks\"][0])\n",
    "    # lat = cloudtrack_data['lat']\n",
    "    # lon = cloudtrack_data['lon']\n",
    "    # lat_resolution = extract_value((lat.max()-lat.min())/len(lat))\n",
    "    # lon_resolution = extract_value((lon.max()-lon.min())/len(lon))\n",
    "    # cloudtrack_data.close()\n",
    "    columns = [\"is_liq\", \"is_mix\", \"is_ice\", \"max_water_frac\",\n",
    "               \"max_ice_fraction\", \"avg_size[km]\", \"max_size[km]\",\n",
    "               \"min_size[km]\", \"avg_size[px]\", \"max_size[px]\",\n",
    "               \"min_size[px]\", \"track_start_time\", \"track_length\",\n",
    "               \"glaciation_start_time\", \"glaciation_end_time\", \"avg_lat\",\n",
    "               \"avg_lon\", \"start_ice_fraction\", \"end_ice_fraction\",\n",
    "               \"ice_frac_hist\", \"lat_hist\", \"lon_hist\",\n",
    "               \"size_hist_km\"]\n",
    "    datapoints_per_cloud = len(columns)\n",
    "    # Iterating through the cloud data\n",
    "    for temp_ind in range(len(config['max_temp_arr'])):\n",
    "        for pole in config['pole_folders']:\n",
    "            min_temp, max_temp = config['min_temp_arr'][temp_ind], config['max_temp_arr'][temp_ind]\n",
    "            temp_key = f'{abs(round(min_temp))}_{abs(round(max_temp))}'\n",
    "            key = f'{pole}_{temp_key}'\n",
    "            cloud_arr = res_dict[key]\n",
    "\n",
    "            cloudinfo_df = pd.DataFrame(\n",
    "                index=range(len(cloud_arr)), columns=columns)\n",
    "            for cloud_ind in range(len(cloud_arr)):\n",
    "                current_cloud = cloud_arr[cloud_ind]\n",
    "                if current_cloud is not None:\n",
    "                    cloudinfo_df.iloc[cloud_ind] = [\n",
    "                        current_cloud.is_liq,\n",
    "                        current_cloud.is_mix,\n",
    "                        current_cloud.is_ice,\n",
    "                        current_cloud.max_water_fraction,\n",
    "                        current_cloud.max_ice_fraction,\n",
    "                        extract_value(current_cloud.avg_cloud_size_km),\n",
    "                        extract_value(current_cloud.max_size_km),\n",
    "                        extract_value(current_cloud.min_size_km),\n",
    "                        extract_value(current_cloud.avg_cloud_size_px),\n",
    "                        extract_value(current_cloud.max_size_px),\n",
    "                        extract_value(current_cloud.min_size_px),\n",
    "                        current_cloud.track_start_time,\n",
    "                        current_cloud.track_length,\n",
    "                        current_cloud.glaciation_start_time,\n",
    "                        current_cloud.glaciation_end_time,\n",
    "                        extract_value(current_cloud.avg_cloud_lat),\n",
    "                        extract_value(current_cloud.avg_cloud_lon),\n",
    "                        current_cloud.start_ice_fraction_arr,\n",
    "                        current_cloud.end_ice_fraction_arr,\n",
    "                        current_cloud.ice_fraction_list,\n",
    "                        current_cloud.lat_list,\n",
    "                        current_cloud.lon_list,\n",
    "                        current_cloud.cloud_size_km_list\n",
    "                    ]\n",
    "\n",
    "            # Ensure output directory exists\n",
    "            output_dir = os.path.join(\n",
    "                config['postprocessing_output_dir'],\n",
    "                config['time_folder_name'],\n",
    "                f\"T_{abs(round(min_temp)):02}_{abs(round(max_temp)):02}_agg_{config['agg_fact']:02}\"\n",
    "            )\n",
    "            os.makedirs(os.path.dirname(output_dir), exist_ok=True)\n",
    "\n",
    "            # Save DataFrame to Parquet\n",
    "            output_dir_parq = output_dir + \".parquet\"\n",
    "            print(\"Writing to \", output_dir_parq)\n",
    "            cloudinfo_df.to_parquet(output_dir_parq)\n",
    "\n",
    "            # Optionally save as CSV\n",
    "            if config['write_csv']:\n",
    "                output_dir_csv = output_dir + \".csv\"\n",
    "                cloudinfo_df.to_csv(output_dir_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing np\n",
      "Analyzing sp\n",
      "Analyzing np 5_0\n",
      "Analyzing sp 5_0\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_05_00.parquet\n",
      "Analyzing np 10_5\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/sp/20240101.0000_20240102.0000/Agg_03_T_05_00.parquet\n",
      "Analyzing sp 10_5\n",
      "Skipping sp -10 to -5\n",
      "Analyzing sp 15_10\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_10_05.parquet\n",
      "Analyzing np 15_10\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_15_10.parquet\n",
      "Analyzing np 20_15\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/sp/20240101.0000_20240102.0000/Agg_03_T_15_10.parquet\n",
      "Analyzing sp 20_15\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_20_15.parquet\n",
      "Analyzing np 25_20\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/sp/20240101.0000_20240102.0000/Agg_03_T_20_15.parquet\n",
      "Analyzing sp 25_20\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_25_20.parquet\n",
      "Analyzing np 30_25\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/sp/20240101.0000_20240102.0000/Agg_03_T_25_20.parquet\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_30_25.parquet\n",
      "Analyzing sp 30_25\n",
      "Analyzing np 35_30\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/np/20240101.0000_20240102.0000/Agg_03_T_35_30.parquet\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/sp/20240101.0000_20240102.0000/Agg_03_T_30_25.parquet\n",
      "Analyzing sp 35_30\n",
      "Writing to  /cluster/work/climate/dnikolo/Cloud_analysis/sp/20240101.0000_20240102.0000/Agg_03_T_35_30.parquet\n"
     ]
    }
   ],
   "source": [
    "config = read_config(\"/cluster/work/climate/dnikolo/n2o/Glaciation_time_estimator/configs/config_testing_2024.yaml\")\n",
    "tracking_fps = generate_tracking_filenames(config)\n",
    "with Manager() as manager:\n",
    "    cloud_dict = manager.dict()\n",
    "    # TODO: Paralelize here\n",
    "    part_analize_single_pole = partial(\n",
    "        analize_single_pole, cloud_dict=cloud_dict, tracking_fps=tracking_fps, config=config)\n",
    "    # part_analize_single_pole(\"np\")\n",
    "    with NestablePool(2) as pool:\n",
    "        pool.map(part_analize_single_pole, config['pole_folders'])\n",
    "        pool.close()\n",
    "\n",
    "# with Manager() as manager:\n",
    "#     cloud_dict = manager.dict()\n",
    "#     # TODO: Paralelize here\n",
    "#     part_analize_single_pole = partial(\n",
    "#         analize_single_pole, cloud_dict=cloud_dict, tracking_fps=tracking_fps, config=config)\n",
    "#     with NestablePool(2) as pool:\n",
    "#         pool.map(part_analize_single_pole, config['pole_folders'])\n",
    "#         pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flex_trkr_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
